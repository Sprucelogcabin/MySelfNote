## 八大排序

![img](https://img-blog.csdnimg.cn/20190206100152611.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI0MDE2MzA5,size_16,color_FFFFFF,t_70)

### **快排**

选择一个基准元素，将比基准元素小的元素放在其前面，比基准元素大的元素放在其后面，然后在将小于基准值元素的子数列和大于基准元素的子数列按原来的方法排序，直到整个序列有序;

#### 快速排序时间复杂度

> 快速排序的时间复杂度在最坏情况下是O(N2)，平均的时间复杂度是O(N*lgN)。

这句话很好理解: 假设被排序的数列中有N个数。遍历一次的时间复杂度是O(N)，需要遍历多少次呢? 至少lg(N+1)次，最多N次。

- 为什么最少是lg(N+1)次? 快速排序是采用的分治法进行遍历的，我们将它看作一棵二叉树，它需要遍历的次数就是二叉树的深度，而根据完全二叉树的定义，它的深度至少是lg(N+1)。因此，快速排序的遍历次数最少是lg(N+1)次。
- 为什么最多是N次? 这个应该非常简单，还是将快速排序看作一棵二叉树，它的深度最大是N。因此，快读排序的遍历次数最多是N次。

**优缺点**

优点：极快数据移动少；

缺点：不稳定；

**效率分析**

此排序算法的效率在序列越乱的时候，效率越高。在数据有序时，会退化成冒泡排序；

**优化方法**

a.当待排序序列的长度分割到一定大小后，使用插入排序；

原因：对于很小和部分有序的数组，快排不如插排好。当待排序序列的长度分割到一定大小后，继续分割的效率比插入排序要差，此时可以使用插排而不是快排；

b.在一次分割结束后，可以把与key相等的元素聚集在一起，继续下次分割时，不必再对于key相等元素分割；

**应用场景**

 a.求数组中第k小的数

将数组中某一个元素m作为划分依据,即m=arr[0]。若m前面的元素个数大于k，则第k小的数一定在m前面的元素中，这时我们只需要继续在m前面的元素中找第k小的数；若m前面的元素小于k，则第k小的数一定在m后面的元素中，这时我们只需要在m后面的元素中找第k小的数；

```java
public class QuickSort {
    public static void main(String[] args) {
        Random random = new Random();
        int[] arr = new int[10];
        for (int i = 0; i < 10; i++) {
            arr[i] = random.nextInt(100);
        }
        System.out.println("排序前："+Arrays.toString(arr));
        quickSort(arr, 0, arr.length - 1);
        System.out.println("排序后：" + Arrays.toString(arr));
    }

    public static void quickSort(int[] arr, int start, int end) {
        if (arr.length < 0) {
            return;
        }
        //如果最左边索引等于右边索引，即数组中只有一个元素，直接返回
        if (start >= end) {
            return;
        }
        int left = start;
        int right = end;
        //选取左边的数为基准数
        int temp = arr[left];
        while (left < right) {
            //right左移，直到遇到比基准数小的
            while (left < right && arr[right] >= temp) {
                right--;
            }
            arr[left] = arr[right];
            //left右移，直到遇到比基准数大的
            while (left < right && arr[left] <= temp) {
                left++;
            }
            arr[right] = arr[left];
        }
        arr[left] = temp;
        //对基准数左边进行排序
        quickSort(arr, start, left - 1);
        //对基准数右边进行排序
        quickSort(arr, left + 1, end);
    }
}

```



### 简单选择排序

(1)从待排序序列中，找到关键字最小的元素；

(2)如果最小元素不是待排序序列的第一个元素，将其和第一个元素互换；

(3)从余下的 N - 1 个元素中，找出关键字最小的元素，重复(1)、(2)步，直到排序结束。

最好情况下，即待排序记录初始状态就已经是升序排列了，则不需要移动记录。

最坏情况下，即待排序记录初始状态是按第一条记录最大，之后的记录从大到小顺序排列，则需要移动记录的次数最多为3（n-1）。简单选择排序过程中需要进行的比较次数与初始状态下待排序的记录序列的排列情况无关。当i=1时，需进行n-1次比较；当i=2时，需进行n-2次比较；依次类推，共需要进行的比较次数是(n-1)+(n-2)+…+2+1=n(n-1)/2，即进行比较操作的时间复杂度为**O(n^2)，进行移动操作的时间复杂度为O(n)**。

简单选择排序是不稳定排序。

**基本思想**

第一趟：从第一个记录开始，将后面n-1个记录进行比较，找到其中最小的记录和第一个记录进行交换；

第二趟：从第二个记录开始，将后面n-2个记录进行比较，找到其中最小的记录和第2个记录进行交换；

...........

第i趟：从第i个记录开始，将后面n-i个记录进行比较，找到其中最小的记录和第i个记录进行交换；

以此类推，经过n-1趟比较，将n-1个记录排到位，剩下一个最大记录直接排在最后；

```java
public static void simpleSort(int[] arr){
    int length = arr.length;
    int index;
    for(int i = 0;i<length;i++){
        index = i;
        //每次找到最小数的索引，并与当前值交换
        for(int j = i+1;j<length;j++){
            if(arr[j]<arr[index]){
                index = j;
            }
        }
        int temp = arr[i];
        arr[i] = arr[index];
        arr[index] = temp;
    }
}

```



### 冒泡排序

（1）**基本原理**

在要排序的一组数中，对当前还未排好序的范围内的全部数，自上而下对相邻的两个数依次进行比较，让较大的数往下沉，较小的往上冒。即：每当两相邻的数比较后发现他们的排序与排序要求相反时，就将他们互换。

（2）**优缺点**

优点：稳定

缺点：慢，每次只能移动两个相邻的数据；

```java
public static void bubbleSort(int[] arr){
        int length = arr.length;
        for(int i = 0;i<length - 1;i++){
            for(int j = 0; j < arr.length - i - 1;j++){
                //若arr[j]>arr[j+1]则下沉
                if(arr[j] > arr[j + 1]){
                    int temp = arr[j];
                    arr[j] = arr[j+1];
                    arr[j+1] = temp;
                }
            }
        }
    }
```



### **插入排序**

（1）**基本思想**

将一个记录插入到已排序好的有序表中，从而得到一个新的，记录数增1的有序表。即先将序列的第一个记录看成是一个有序的子序列，然后从第二个记录逐个进行插入，直至整个序列有序为止。

（2）优缺点

优点：稳定，快

缺点：比较次数不一定，比较次数越少，插入点后的数据移动越多，特别是数据量庞大的时候

```java
public static void insertSort(int[] arr){
        int length = arr.length;
        for(int i = 1;i < length;i++){
            //每次将一个新的数据插入到一个有序数组中
            for(int j = i;j > 0;j--){
                if(arr[j] < arr[j-1]){
                    int temp = arr[j];
                    arr[j] = arr[j-1];
                    arr[j-1] = temp;
                }else{
                    break;
                }
            }
        }
    }
```



### 基数排序

基数排序（radix sort）属于“分配式排序”（distribution sort），又称“桶子法”（bucket sort）或bin sort，顾名思义，它是透过键值的部份资讯，将要排序的==元素分配==至某些“桶”中，藉以达到排序的作用，基数排序法是属于==稳定性==的排序，其==时间复杂度==为O (nlog(r)m)，其中r为所采取的基数，而m为堆数，在某些时候，基数排序法的效率高于其它的稳定性排序法。

```java
public class RadixSort {
    public static void main(String[] args) {
        Random random = new Random();
        int[] arr = new int[10];
        for (int i = 0; i < arr.length; i++) {
            arr[i] = random.nextInt(100);
        }
        System.out.println("排序前："+ Arrays.toString(arr));
        sort(arr,2);
        System.out.println("排序后："+ Arrays.toString(arr));
    }

    public static void sort(int[] arr,int d){
        //参数d代表最大的数有多少位
        //k相当于一个计数器，记录放到数组哪个位置
        int k = 0;
        int n = 1;
        //控制键值排序依据在哪一位
        int m = 1;
        //数组的第一位表示可能的余数0-9
        int[][] temp = new int[10][arr.length];
        //数组order[i]用来表示该位是i的数的个数
        int[] order = new int[10];
        while (m <= d){
            //依次将余数为t的数放入第t行第order[t]的位置
            for (int i = 0; i < arr.length; i++) {
                int t = (arr[i]/n)%10;
                temp[t][order[t]] = arr[i];
                order[t]++;
            }
            //依次取出每一个桶中的元素，放入数组中的新位置
            for(int i = 0;i < 10;i++){
                if(order[i] != 0){
                    for(int j = 0;j < order[i];j++){
                        arr[k] = temp[i][j];
                        k++;
                    }
                }
                order[i] = 0;
            }
            n *= 10;
            k = 0;
            m++;
        }
    }
}

```



### 堆排序

4.1、**二叉堆定义**

二叉堆是完全二叉树或近似完全二叉树。二叉堆满足两个特性:

（1）父结点的键值总是大于或者等于（小于或者等于）任何一个子节点的键值；

（2）每个结点的左子树和右子树都是一个二叉堆；

当父结点的键值总是大于或者等于任何一个子节点的键值时为大根堆。当父结点的键值总是小于或等于任何一个子节点的键值时为小根堆；

4.2、**堆的存储**

一般都用数组来表示堆，i结点的父结点下标就为（i-1）/2.它的左右子节点的下标分别为2*i+1和2*i+2.

![img](https://gitee.com/shen1shen1/pic-md1/raw/master/img/20211003201923.jpeg)

4.3、堆的插入：

每次插入都是将新数据放在数组最后。可以发现从这个新数据的父结点到根结点必然为一个有序的数列，然后将这个新数据插入到这个有序数据中

（1）**用大根堆排序的基本思想**

先将初始数组建成一个大根堆，此堆为初始的无序区；

再将最大的元素和无序区的最后一个记录交换，由此得到新的无序区和有序区，且满足<=的值；

由于交换后新的根可能违反堆性质，故将当前无序区调整为堆。然后再次将其中最大的元素和该区间的最后一个记录交换，由此得到新的无序区和有序区，且仍满足关系的值<=的值，同样要将其调整为堆；

..........

直到无序区只有一个元素为止；

4.4：**应用**

寻找M个数中的前K个最小的数并保持有序；

时间复杂度：O(K)[创建K个元素最大堆的时间复杂度] +（M-K）*log(K)[对剩余M-K个数据进行比较并每次对最大堆进行从新最大堆化]

```java
public class HeapSort {
    //堆排序，从小到大
    public static void main(String[] args) {
        Random random = new Random();
        int[] arr = new int[10];
        for(int i=0;i<arr.length;i++){
            arr[i] = random.nextInt(50);
        }

        //构建大顶堆
        for(int i = (arr.length-1)/2;i>=0;i--){
            adjustHead(arr,arr.length,i);
        }
        System.out.println(Arrays.toString(arr));
        //逐次调整，将堆顶与最后一个元素交换，把大的元素放在后边
        for(int i = arr.length-1;i>0;i--){
            swap(arr,0,i);
            adjustHead(arr,i,0);
        }

        System.out.println(Arrays.toString(arr));
    }

    public static void adjustHead(int[] arr,int size,int index){
        int leftIndex = index*2+1;
        int rightIndex = index*2+2;
        int minIndex = index;
        //如果左边的比较大，就交换
        if(leftIndex < size && arr[leftIndex] > arr[minIndex]){
            minIndex = leftIndex;
        }
        //如果右边比较大，就交换
        if(rightIndex < size && arr[rightIndex] > arr[minIndex]){
            minIndex = rightIndex;
        }
        //交换，并调整子堆
        if(minIndex != index){
            swap(arr,index,minIndex);
            adjustHead(arr,size,minIndex);
        }
    }

    public static void swap(int[] arr, int i,int j){
        int temp = arr[i];
        arr[i] = arr[j];
        arr[j] = temp;
    }
    
}

```



### **希尔排序**

（1）**基本思想**

先将整个待排序元素序列分割成若干子序列（由相隔某个“增量”的元素组成的）分别进行直接插入排序，然后依次缩减增量再进行排序，待整个序列中的元素基本有序（增量足够小）时，再对全体元素进行一次直接插入排序（因为直接插入排序在元素基本有序的情况下，效率很高）；

（2）**适用场景**

比较在希尔排序中是最主要的操作，而不是交换。用已知最好的步长序列的希尔排序比直接插入排序要快，甚至在小数组中比快速排序和堆排序还快，但在涉及大量数据时希尔排序还是不如快排；

```java
public class ShellSort {
    public static void main(String[] args) {
        Random random = new Random();
        int[] arr = new int[10];
        for (int i = 0; i < arr.length; i++) {
           arr[i] = random.nextInt(100);
        }
        System.out.println("排序前"+ Arrays.toString(arr));
        shellSort(arr);
        System.out.println("排序后："+Arrays.toString(arr));
    }

    public static void shellSort(int[] arr){
        //步长
        int gap = arr.length;
        while (true){
            //步长每次减半
            gap /= 2;
            for(int i = 0;i < gap;i++){
                //使用简单插入排序
                for(int j = i+gap;j<arr.length;j+=gap){
                    int k = j - gap;
                    while (k >= 0 && arr[k] > arr[k+gap]){
                        int temp = arr[k];
                        arr[k] = arr[k+gap];
                        arr[k+gap] = temp;
                        k -= gap;
                    }
                }
            }
            if(gap == 1){
                break;
            }
        }
    }
}

```



### **归并排序**

**（1）基本思想**

首先将初始序列的n个记录看成是n个有序的子序列，每个子序列的长度为1，然后两两归并，得到n/2个长度为2的有序子序列，在此基础上，再对长度为2的有序子序列进行两两归并，得到若干个长度为4的有序子序列，以此类推，直到得到一个长度为n的有序序列为止；

***（2）适用场景***

若n较大，并且要求排序稳定，则可以选择归并排序。

```java
public class MergeSort {
    public static void main(String[] args) {
        Random random = new Random();
        int[] nums = new int[10];
        for(int i = 0;i<nums.length;i++){
            nums[i] = random.nextInt(100);
        }
        System.out.println("排序前："+Arrays.toString(nums));
        sort(nums,0,nums.length-1);
        System.out.println("排序后："+Arrays.toString(nums));
    }

    public static int[] sort(int[] nums,int low,int high){
        int mid = (low + high)/2;
        if(low<high){
            //排序左边
            sort(nums,low,mid);
            //排序右边
            sort(nums,mid+1,high);
            //左右归并
            merge(nums,low,mid,high);

        }
        return nums;
    }

    public static void merge(int[] nums,int low,int mid,int high){
        int[] temp = new int[high-low+1];
        int i = low;
        int j = mid + 1;
        int k = 0;
        //把较小的数先移到新数组中
        while(i<=mid && j<=high){
            if(nums[i]<nums[j]){
                temp[k++] = nums[i++];
            }else {
                temp[k++] = nums[j++];
            }
        }
        //把左边剩余的数移入数组
        while(i<=mid){
            temp[k++] = nums[i++];
        }
        //把右边剩余的数移入数组
        while (j<=high){
            temp[k++] = nums[j++];
        }
        //把新数组中的数覆盖nums数组
        for(int x = 0;x<temp.length;x++){
            nums[x+low] = temp[x];
        }
    }
}
```



### 总结

一、稳定性:

　  稳定：冒泡排序、插入排序、归并排序和基数排序

　　不稳定：选择排序、快速排序、希尔排序、堆排序

二、平均时间复杂度

　　O(n^2):直接插入排序，简单选择排序，冒泡排序。

　　在数据规模较小时（9W内），直接插入排序，简单选择排序差不多。当数据较大时，冒泡排序算法的时间代价最高。性能为O(n^2)的算法基本上是相邻元素进行比较，基本上都是稳定的。

　　O(nlogn):快速排序，归并排序，希尔排序，堆排序。

　　其中，快排是最好的， 其次是归并和希尔，堆排序在数据量很大时效果明显。

三、排序算法的选择

　　1.数据规模较小

 　　（1）待排序列基本序的情况下，可以选择**直接插入排序**；

 　　（2）对稳定性不作要求宜用简单选择排序，对稳定性有要求宜用插入或冒泡

　　2.数据规模不是很大

　　（1）完全可以用内存空间，序列杂乱无序，对稳定性没有要求，**快速排序**，此时要付出log（N）的额外空间。

　　（2）序列本身可能有序，对稳定性有要求，空间允许下，宜用归并排序

　　3.数据规模很大

  　　（1）对稳定性有求，则可考虑归并排序。

  　　（2）对稳定性没要求，宜用堆排序

　　4.序列初始基本有序（正序），宜用直接插入，冒泡

## 链表反置

```
public ListNode Rever() {
		ListNode preNode = null;
		ListNode nextNode = null;
		while(head!=null) {
			nextNode = head.getNext();
			head.setNext(preNode);
			preNode = head;
			head = nextNode;
		}
		return preNode;
	}
```

# Linux和GIT

## Linux

**Linux查看端口占用情况**

> - netstat  -anp|grep  端口号 

> 方法一：
>
> 1.先用ps -ef | grep xxx(某个进程)，可以查看某个进程的pid。
>
> 2.再用netstat -anp | grep pid号，可以查看到该进程占用的端口号!
>
> 方法二：
>
> 直接用**lsof**命令可以查看端口使用情况!

 **如何查看磁盘还有多少剩余空间** 

 这里主要可以用 df -ah 命令来查看，df 是用来查看文件系统磁盘空间使用情况的命令，-a 显示所有文件系统，-h 用人们可读的方式进行显示。 

**如何查看某个进程对 CPU 的使用情况**

1） 可以使用 top 命令

top命令是Linux下常用的性能分析工具，能够实时显示系统中各个进程的资源占用状况，类似于Windows的任务管理器。

2） 使用 ps 命令

ps aux 可以显示所有使用者的进程，最常用的方法是ps aux，然后再利用一个管道符号导向到grep去查找特定的进程。比如查看nginx 进程可以用 

ps aux | grep nginx

**查看当前系统正在运行进程**

ps -ef / ps -aux

**查找正在执行的tomcat** 

ps -ef | grep tomcat 

### 一些基本的指令

cd / ： 返回到根目录
cd ~ ：返回到用户的主目录
cd ../ ： 返回上一级
内容：蓝色的是文件夹，白色的是文件，青蓝色是软连接（快捷方式），红色的是压缩包
mkdir ：创建文件夹
touch：创建文件
vi ：创建文件并编辑文件（a：进入编辑；esc退出编辑）
:q! ：不保存直接退出
:wq! :保存并退出
rm -rf ：删除
cat 文件名 ：查看文件（cat 是要一次性显示出来全部信息，当文件很大的时候容易卡顿死机）
more 文件名：查看文件（more 查看的时候，会分批查看）
**tail -x 文件名（查看文件x条信息）**
**tail -f 文件（查看文件的实时变化情况）**
ctrl + c 在linux中表示中断
yum 安装jdk1.8（ yum install -y java-1.8.0-openjdk-devel.x86_64）
sz 文件名 ：下载文件
（命令都是shell语言）
chmod 777 *：修改权限的作用（ 777分别代表用户可读可写可执行的作用）
find / -name 文件名 ：查找文件
mv ：剪切
ps -ef | grep tomcat ：查找正在执行的tomcat 
top ：查找那个性能消耗高（可以在任何目录下使用，ctrl + c 中断）



杀死进程：

配合ps指令使用

**方法二: 通过kill 进程id的方式可以实现,**

首先需要知道进程id, 例如,想要杀死firefox的进程,通过 ps -ef|grep firefox,可以查到firefox的进程id:

然后通过 kill 3781 就可以关闭进程了.

补充: 1. kill -9 来强制终止退出, 例如: kill -9 3781

**方法三: killall 通过程序的名字,来杀死进程**

例如: killall firefox
注意: 该命令可以使用 -9 参数来强制杀死进程, killall -9 firefox

方法四: pkill 通过程序的名字, 直接杀死所有进程
例如: pkill firefox

**方法五: 通过xkill 可以杀死图形程序应用, 例如firefox崩溃无响应,可以使用该命令.**
例如: 用法xkill , 会出现一个白色的x, 然后用鼠标单击想要杀死的应用,如
————————————————

### linux项目部署

**简单的**

将程序打包成war包（通过代码生成），通过rz命令上传到tomcat的webapp文件夹底下，tomcat会自动解压，然后直接运行tomcat就行

**用docker的**

将程序打包成war包，通过rz传到docker容器里面，（docker里面的vi和rz命令需要下载），然后重启tomcat，（docker里面有tomcat镜像，支持部署多个tomcat）

## Git

### git 相关命令

1. \# 下载一个项目和它的整个代码历史

   $ git clone 

   1. \# 添加指定文件到暂存区

      $ git add [file1] [file2] ...

      1. \# 提交暂存区到仓库区

         $ git commit -m [message]

      1. \# 列出所有本地分支

         $ git branch

         1.  新建一个分支，并切换到该分支

            $ git checkout -b [branch]

         1. \# 合并指定分支到当前分支

            $ git merge [branch]

### git merge/rebase

#### Merge

当你运行 git merge 时，你的 HEAD 分支会生成一个新的提交，并保留每个提交历史的祖先。

![img](https://pic3.zhimg.com/80/v2-2f63458fe8dac6f680faf9c1fa4757a2_1440w.jpg)


Fast forward merge是一种不创建提交的合并类型，会更新分支指针到上一次提交。

\#### 优点

\- 使用简单，易于理解。
\- 保持源分支的原始上下文。
\- 源分支上的提交与其他分支的提交是分开的。
\- 可以保留提交历史。

\#### 缺点

\- 乱

#### Rebase

Rebase是将一个分支的修改重写到另一个分支上，而不需要创建新的提交。

你在特性分支上的每一个提交，都会在主分支上创建一个新的提交。这看起来就像这些提交一直是写在主分支之上的一样。

![img](https://pic1.zhimg.com/80/v2-2a15d46ba3e165689147a223570cd4dc_1440w.jpg)

\#### 优点

\- 代码历史是简化的、线性的、可读的。
\- 与许多独立的特性分支的提交历史相比，操作单个提交历史更容易。
\- 干净、清晰的提交信息可以更好地跟踪一个bug或何时引入的某个功能。可以避免众多的单行提交污染历史。

\#### 缺点

\- 会更改历史提交时间，可能会丢失上下文。

# 简历知识点

## map的实现

hashmap，treeMap，hashtable，currenthashmap

### HashMap常用的方法

1: put方法: put(key，value)，我们经常用存储一些常用的数据，比如flag、百分比之类的，我们就可以返回map结构，如果key相同则值会覆盖，允许key和value为null。
2: get方法: get(key)，主要用来取map中存储的数据，我们根据其key值，可以取到对应的value值，没有该key对应的值则返回null。
3: remove方法: remove(key)，主要用来删除map中对应的key及其value值。
4: clear方法，用法: clear()，会清空map中的数据。
5: containsKey(key)，判断map集合中是否包含某个key。6: containsKey(value)，判断map集合中是否包含某个value。
7: entrySet(): hashmap.entrySet().iterator()，entrySet()的效率比keySet)要高。key和value存储在entry对象里面，遍历的时候，拿到entry对象就可以取到value了。
8: keySet(): hashmap.keySet().iterator()，keySet是把key放到一个set集合中，通过迭代器遍历，再用hastmap.get(key)来取到value的值。

### HashMap，

（1）哈希表：在哈希表中进行添加，删除，查找等操作，性能十分之高，不考虑哈希冲突的情况下（后面会探讨下哈希冲突的情况），仅需一次定位即可完成，时间复杂度为O(1)，

（2）**哈希表的主干就是数组**。

**比如我们要新增或查找某个元素，我们通过把当前元素的关键字 通过某个函数映射到数组中的某个位置，通过数组下标一次定位就可完成操作。**

这个函数可以简单描述为：**存储位置 = f(关键字)** ，这个函数f一般称为哈希函数，这个函数的设计好坏会直接影响到哈希表的优劣。

（3）**如果两个不同的元素，通过哈希函数得出的实际存储地址相同怎么办**？也就是说，当我们对某个元素进行哈希运算，得到一个存储地址，然后要进行插入的时候，发现已经被其他元素占用了，其实这就是所谓的**哈希冲突**，也叫**哈希碰撞**。哈希冲突的解决方案有多种:开放定址法（发生冲突，继续寻找下一块未被占用的存储地址），再散列函数法，链地址法，而HashMap即是采用了**链地址法**，也就是**数组+链表**的方式。

## HashMap底层源码

JDK1.8 之前 HashMap 底层是 **数组和链表** 结合在⼀起使⽤也就是 链表散列。 HashMap 通过 key 的hashCode 经过扰动函数处理过后得到 hash 值，然后通过 (n - 1) & hash 判断当前元素存放的位置，如果定位到的数组位置不含链表，就可以直接插入，时间复杂度为O(1)，如果当前位置存在元素的话，就判断该元素与要存⼊的元素的 hash值以及 key 是否相同，如果相同的话，直接覆盖，不相同就通过拉链法解决冲突。链表出现的越少，HashMap性能越高。

相⽐于之前的版本， JDK1.8 之后在解决哈希冲突时有了较⼤的变化，当链表⻓度⼤于阈值（默认为8）（将链表转换成红⿊树前会进行一个判断，如果当前数组的⻓度⼩于 64，那么会选择先进⾏数组扩容，⽽不是转换为红⿊树）时，将链表转化为红⿊树，以减少搜索时间。

#### 1.7和1.8的区别

* HashMap在1.7中是由 **数组＋链表**  也可以说 **哈希表＋链表** 实现的,

  HashMap在1.8中是由 **数组＋链表** +**红黑树** 

* 在**JDK1.7**中，HashMap存储的是**Entry对象**

  在**JDK1.8**当中，HashMap存储的是**实现Entry接口**的**Node对象**

* **JDK1.7**用的是**头插法**，而**JDK1.8及之后**使用的都是**尾插法**

  * **为什么从头插法改成尾插法？**

    在1.7中，是没有红黑树的，在并发的情况下单链表过长**，**会成环**，**发生死循环

    在1.8中，尾插法就可以解决这个问题

  - 1.8也没有解决数据覆盖的问题

* **扩容机制**

  * 在**JDK1.7的时候是先扩容后插入**的，这样就会导致无论**这一次插入是不是发生hash冲突都需要进行扩容**，如果这次插入的并**没有发生Hash冲突**的话，那么就会造成**一次无效扩容**，

    但是**在1.8的时候是先插入再扩容**的，优点是可以**减少1.7的一次无效的扩容**，因为如果这次插入没有发生Hash冲突的话，那么其实就不会造成扩容

  * **扩容过程不一样（上面）**

#### HashMap中获取hash值的方法

>  首先判断 key 是否为 null，为 null 则返回 0 ，所以 key 为空的元素对应的[数组](https://so.csdn.net/so/search?q=数组&spm=1001.2101.3001.7020)坐标一定是 0，而且根据 put 会覆盖相同 key 的逻辑来思考，key 为空的元素最多只有一个。不为 null 则返回 key 的 hashCode异或上它的高16位。 

```
static final int hash(Object key) {
    int h;
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}
```

#### HashMap扩容

#### 进行位运算的原因

> (h >>> 16)是无符号右移16位的运算，右边补0，得到 hashCode 的高16位
> (h = key.hashCode()) ^ (h >>> 16) 把 hashCode 和它的高16位进行异或运算，可以使得到的 hash 值更加散列，极可能减少哈希冲突，提升性能。
> 而这么来看 hashCode 被散列 (异或) 的是低16位。原因是获取 key 所对应数组下标的方式是 hash值 % 数组长度。比如 10001 % 101 取余的结果只会看前面二进制数的低三位，其余高位不影响取余结果。而 HashMap 数组长度一般不会超过2的16次幂，那么高16位在大多数情况是用不到的，所以只需要拿 key 的 HashCode 和它的高16位做异或即可让hash值更加散列。

#### 为什么链表出现的越少，HashMap性能越高

简单来说，**HashMap由数组+链表组成的，数组是HashMap的主体**，链表则是主要为了解决哈希冲突而存在的，如果定位到的数组位置不含链表（当前entry的next指向null）,那么查找，添加等操作很快，仅需一次寻址即可；如果定位到的数组包含链表，对于添加操作，其时间复杂度为O(n)，首先遍历链表，存在即覆盖，否则新增；对于查找操作来讲，仍需遍历链表，然后通过key对象的equals方法逐一比对查找。所以，性能考虑，HashMap中的链表出现越少，性能才会越好。

#### put的流程（1.8）

1.通过**hash函数计算key的hash值**，调用putVal方法

2.如果hash表为空，调用**resize()方法创建一个hash表**

3.根据**hash值索引hash表对应桶位置，判断该位置是否有hash碰撞**

　　3.1 **没有碰撞，直接插入映射入hash表**

　　3.2 **有碰撞，遍历桶中节点**

　　　　3.2.1 第一个节点匹配，记录该节点

　　　　3.2.2 第一个节点没有匹配，桶中结构为红黑树结构，按照红黑树结构添加数据，记录返回值

　　　　3.2.3 第一个节点没有匹配，桶中结构是链表结构。遍历链表，找到key映射节点，记录，退出循环。
　　　　　　没有则在链表尾部添加节点。插入后判断链表长度是否大于转换为红黑树要求，符合则转为红黑树结构

　　　　3.2.4 用于记录的值判断是否为null，不为空则是需要插入的映射key在hash表中原来有，替换值，返回旧值putValue方法结束

![image-20210807160731433](https://gitee.com/shen1shen1/pic-md1/raw/master/image-20210807160731433.png)

**总结：**HashMap在put方法中，它使用hashCode()和equals()方法。当我们通过传递key-value对调用put方法的时候，HashMap使用Key hashCode()和哈希算法来找出存储key-value对的索引。如果索引处为空，则直接插入到对应的数组中，否则，判断是否是红黑树，若是，则红黑树插入，否则遍历链表，若长度不小于8，则将链表转为红黑树，转成功之后 再插入。

#### 为什么HashMap的容量大小要取2的指数倍？

有两个原因：**1**，**提升计算效率**：因为2的指数倍的二进制都是只有一个1，而2的指数倍-1的二进制就都是左全0右全1。那么跟（2^n - 1）做按位与运算的话**，**得到的值就一定在【0,（2^n - 1）】区间内**，**这样的数就刚合适可以用来作为哈希表的容量大小，因为往哈希表里插入数据，就是要对其容量大小取余，从而得到下标。所以用2^n做为容量大小的话，就可以用按位与操作替代取余操作，提升计算效率**。**2.**便于动态扩容后的重新计算哈希位置时能均匀分布元素**：因为动态扩容仍然是按照2的指数倍，所以按位与操作的值的变化就是二进制高位+1，比如16扩容到32，二进制变化就是从0000 1111（即15）到0001 1111（即31）**，**那么这种变化就会使得需要扩容的元素的哈希值重新按位与操作之后所得的下标值要么不变，要么+16（即挪动扩容后容量的一半的位置），这样就能使得原本在同一个链表上的元素均匀（相隔扩容后的容量的一半）分布到新的哈希表中。

#### 线程安全HashMap

- **Hashtable线程安全但效率低下**

Hashtable容器使用synchronized来保证线程安全，但在线程竞争激烈的情况下Hashtable的效率非常低下。因为当一个线程访问Hashtable的同步方法时，其他线程访问Hashtable的同步方法时，可能会进入阻塞或轮询状态。如线程1使用put进行添加元素，线程2不但不能使用put方法添加元素，并且也不能使用get方法来获取元素，所以竞争越激烈效率越低。

**解决**

**分段锁（ConcurrentHashMap所使用的锁分段技术）**

HashTable容器在竞争激烈的并发环境下表现出效率低下的原因，是因为所有访问HashTable的线程都必须竞争同一把锁，那假如容器里有多把锁，每一把锁用于锁容器其中一部分数据，那么当多线程访问容器里不同数据段的数据时，线程间就不会存在锁竞争，从而可以有效的提高并发访问效率，这就是**ConcurrentHashMap所使用的锁分段技术**，

##### ConcurrentHashMap（JDK1.7）

**JDK1.7的时候ConcurrentHashMap 是由 Segment 数组结构和 HashEntry 数组结构组成**。**首先将数据分成一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数据也能被其他线程访问。**有些方法需要跨段，比如size()和containsValue()，它们可能需要锁定整个表而而不仅仅是某个段，这需要**按顺序**锁定所有段，操作完毕后，又**按顺序**释放所有段的锁。这里“按顺序”是很重要的，否则极有可能出现死锁，在ConcurrentHashMap内部，段数组是final的，并且其成员变量实际上也是final的，但是，仅仅是将数组声明为final的并不保证数组成员也是final的，这需要实现上的保证。这可以确保不会出现死锁，因为获得锁的顺序是固定的。
 ConcurrentHashMap是由Segment数组结构和HashEntry数组结构组成。Segment是一种可重入锁ReentrantLock，在ConcurrentHashMap里扮演锁的角色，HashEntry则用于存储键值对数据。一个ConcurrentHashMap里包含一个Segment数组，Segment的结构和HashMap类似，是一种数组和链表结构， 一个Segment里包含一个HashEntry数组，每个HashEntry是一个链表结构的元素， 每个Segment守护者一个HashEntry数组里的元素,当对HashEntry数组的数据进行修改时，必须首先获得它对应的Segment锁。

![img](https:////upload-images.jianshu.io/upload_images/17755742-0aeb208cbf2192f9.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/502/format/webp)

#### **LinkedHashMap**

LinkedHashMap的概述: Map 接口的哈希表和链接列表实现，具有可预知的迭代顺序LinkedHashMap的特点： 底层的数据结构是链表和哈希表 元素有序 并且唯一
元素的有序性由链表数据结构保证 唯一性由 哈希表数据结构保证
Map集合的数据结构只和键有关

#### TreeMap集合

TreeMap 键不允许插入null
TreeMap: 键的数据结构是红黑树,可保证键的排序和唯一性
排序分为自然排序和比较器排序
线程是不安全的效率比较高
TreeMap集合排序：
实现Comparable接口，重写CompareTo方法
使用比较器

#### HashTable

Hashtable继承Map接口，同样实现一个key-value映射的哈希表。其数据结构同样基于数组加链表，任何非空（non-null）的对象都可作为key或者value。

Hashtable通过initial capacity和load factor两个参数调整性能。通常缺省的load factor 0.75也和hashMap相同。

**由于作为key的对象将通过获取它的hashcode来确定与之对应的value的位置，因此任何作为key的对象都必须实现hashCode和equals方法，因为HashTable直接调用对象的方法。**hashCode和equals方法继承自根类Object，如果你用自定义的类当作key的话，要相当小心，按照散列函数的定义，如果两个对象相 同，即obj1.equals(obj2)=true，则它们的hashCode必须相同，但如果两个对象不同，则它们的hashCode不一定不同，

如果两个不同对象的hashCode相同，这种现象称为冲突，即两个对象在同一链表上存储，冲突会导致操作哈希表的时间开销增大，所以尽量定义好的hashCode()方法，能加快哈希表的操作。

如果相同的对象有不同的hashCode，对哈希表的操作会出现意想不到的结果，要避免这种问题，要牢记一条：要同时覆写equals方法和hashCode方法，而不是只写其中一个。

HashTable是线程安全的，和HashMap不同

HashTable在不指定容量的情况下的默认容量为11，而HashMap为16，Hashtable不要求底层数组的容量一定是为2的n次方，而HashMap则一定为2的n次方。

Hashtable扩容时，将容量变为原来的2倍加1，而HashMap扩容时，将容量变为原来的2倍

### List

List是Java中比较常用的集合类，关于List接口有很多实现类，本文就来简单介绍下其中几个重点的实现ArrayList、LinkedList和Vector之间的关系和区别。

List 是一个接口，它继承于Collection的接口。它代表着有序的队列。当我们讨论List的时候，一般都和Set作比较。

**ArrayList**

ArrayList底层是用数组实现的，可以认为ArrayList是一个可改变大小的数组。随着越来越多的元素被添加到ArrayList中，其规模是动态增加的。

**LinkedList**

LinkedList底层是通过双向链表实现的。所以，LinkedList和ArrayList之前的区别主要就是数组和链表的区别。

> 数组中查询和赋值比较快，因为可以直接通过数组下标访问指定位置。
>
> 
>
> 链表中删除和增加比较快，因为可以直接通过修改链表的指针（Java中并无指针，这里可以简单理解为指针。其实是通过Node节点中的变量指定）进行元素的增删。

所以，LinkedList和ArrayList相比，增删的速度较快。但是查询和修改值的速度较慢。同时，LinkedList还实现了Queue接口，所以他还提供了offer(), peek(), poll()等方法。

**Vector**

Vector和ArrayList一样，都是通过数组实现的，但是Vector是线程安全的。和ArrayList相比，其中的很多方法都通过同步（synchronized）处理来保证线程安全。

如果你的程序不涉及到线程安全问题，那么使用ArrayList是更好的选择（因为Vector使用synchronized，必然会影响效率）。

二者之间还有一个区别，就是扩容策略不一样。在List被第一次创建的时候，会有一个初始大小，随着不断向List中增加元素，当List认为容量不够的时候就会进行扩容。Vector缺省情况下自动增长原来一倍的数组长度，ArrayList增长原来的50%。



### LinkedList，ArrayList

**如何选择**

如果涉及到多线程，那么就选择Vector（当然，你也可以使用ArrayList并自己实现同步）。

如果不涉及到多线程就从LinkedList、ArrayList中选。 LinkedList更适合从中间插入或者删除（链表的特性）。 ArrayList更适合检索和在末尾插入或删除（数组的特性）。

- 

#  https请求流程

1.客户端想服务器发起HTTPS的请求，连接到服务器的443端口；

2.服务器将非对称加密的公钥传递给客户端，以证书的形式回传到客户端

3.客户端接受到该公钥进行验证（CA顶级证书是一出场就有的，操作系统镜像里边是有的），（CA证书组成*公钥、公钥拥有者名称、CA 的数字签名、有效期、授权中心名称、证书序列号等信息*）就是验证2中证书，如果有问题，则HTTPS请求无法继续；如果没有问题，则上述公钥是合格的。（第一次HTTP请求）客户端这个时候随机生成一个(对称加密的密钥)，私钥，成为client key,客户端私钥，用于对称加密数据的。使用前面的公钥对client key进行非对称加密；

4.进行二次HTTP请求，将加密之后的client key传递给服务器；

5.服务器使用私钥进行解密，得到client key,使用client key对数据进行对称加密

6.将对称加密的数据传递给客户端，客户端使用对称解密，得到服务器发送的数据，完成第二次HTTP请求。

### linux系统常用命令

```
# 与用户或用户组名有关的参数：
-user name : 列出文件所有者为name的文件
-group name : 列出文件所属用户组为name的文件
-uid n : 列出文件所有者为用户ID为n的文件
-gid n : 列出文件所属用户组为用户组ID为n的文件
```

```
 与文件权限及名称有关的参数：
-name filename ：找出文件名为filename的文件
-size [+-]SIZE ：找出比SIZE还要大（+）或小（-）的文件
-tpye TYPE ：查找文件的类型为TYPE的文件，TYPE的值主要有：一般文件（f)、设备文件（b、c）、
             目录（d）、连接文件（l）、socket（s）、FIFO管道文件（p）；
-perm mode ：查找文件权限刚好等于mode的文件，mode用数字表示，如0755；
-perm -mode ：查找文件权限必须要全部包括mode权限的文件，mode用数字表示
-perm +mode ：查找文件权限包含任一mode的权限的文件，mode用数字表示
```

# 框架

### 简述SSM框架

SSM框架的基本原理分三层解释为：

**SpringMVC：**

1.客户端发送请求到DispacherServlet（分发器）

2.由DispacherServlet控制器查询HanderMapping，找到处理请求的Controller

3.Controller调用业务逻辑处理后，返回ModelAndView

4.DispacherSerclet查询视图解析器，找到ModelAndView指定的视图

5.视图负责将结果显示到客户端

**Spring：**

我们平时开发接触最多的估计就是IOC容器，它可以装载bean（也就是我们Java中的类，当然也包括service dao里面的），有了这个机制，我们就不用在每次使用这个类的时候为它初始化，很少看到关键字new。另外spring的aop，事务管理等等都是我们经常用到的。

**Mybatis：**

mybatis是对jdbc的封装，它让数据库底层操作变的透明。mybatis的操作都是围绕一个sqlSessionFactory实例展开的。mybatis通过配置文件关联到各实体类的Mapper文件，Mapper文件中配置了每个类对数据库所需进行的sql语句映射。在每次与数据库交互时，通过sqlSessionFactory拿到一个sqlSession，再执行sql命令

### Spring和SpringMVC的区别

首先 springmvc和spring它俩都是容器，容器就是管理对象的地方，例如Tomcat，就是管理servlet对象的，而springMVC容器和spring容器，就是管理bean对象的地方，再说的直白点，springmvc就是管理controller对象的容器，spring就是管理service和dao的容器，这下你明白了吧。所以我们在springmvc的配置文件里配置的扫描路径就是controller的路径，而spring的配置文件里自然配的就是service和dao的路径 

其次， spring容器和springmvc容器的关系是父子容器的关系。spring容器是父容器，springmvc是子容器。在子容器里可以访问父容器里的对象，但是在父容器里不可以访问子容器的对象，说的通俗点就是，在controller里可以访问service对象，但是在service里不可以访问controller对象

　　所以这么看的话，所有的bean，都是被spring或者springmvc容器管理的，他们可以直接注入。然后springMVC的拦截器也是springmvc容器管理的，所以在springmvc的拦截器里，可以直接注入bean对象。

## Spring和SpringMVC的区别

springboot是springmvc的加强版，少一层servlet,有自己的主方法，不需要依赖tomcat，可以自己打成一个jar包去运行，也可以打成一个war包交给tomcat去执行
springmvc没有自己的主方法，必须依赖tomcat执行，识别需要反射和代理
很多比tomcat出现的晚的注解tomcat是无法识别的，所以需要自己写，springmvc的jar包包含识别注解的代码，需要想办法别人触发（如：web.xml）

### IOC和AOP

#### IOC:

控制反转，是一种降低对象之间耦合关系的设计思想，它的**主要目的**是借助于“第三方”(Spring 中的 IOC 容器) 实现具有依赖关系的对象之间的解耦(IOC容器管理对象，你只管使用即可)，从而降低代码之间的耦合度。

面试的时候最好能说出来个**例子**，加深理解。例子：租房子，以前租房子需要一个房子一个房子找，费时费力，然后现在加入一个房屋中介，把你需要的房型告诉中介，就可以直接选到需要的房子，中介就相当于spring容器。

实现原理简述：

- 创建xml配置文件，配置要创建的对象类
- 通过反射创建实例
- 获取需要注入的接口实现类并将其赋值给该接口

IOC（控制反转），这是spring的核心，他将对象的创建和调用交由了spring进行管理。你只需提供你需要哪个对象，他便能帮你创建好，你不必去关心实现的细节等。就好比你通过中介找对象一样，只需提要求，中介就会吧符合要求的姑娘推给你供你使用。所以类的创建、销毁都是由spring来控制，也就是说控制对象生存周期的不在是引用他的对象，而是spring。 **IoC 容器是 Spring 用来实现 IoC 的载体，  IoC 容器实际上就是个Map（key，value）,Map 中存放的是各种对象。** **IoC 容器就像是一个工厂一样，当我们需要创建一个对象的时候，只需要配置好配置文件/注解即可，完全不用考虑对象是如何被创建出来的。**实际项目中一个 Service 类可能有几百甚至上千个类作为它的底层，假如我们需要实例化这个 Service，你可能要每次都要搞清这个 Service 所有底层类的构造函数，这可能会把人逼疯。如果利用 IoC 的话，你只需要配置好，然后在需要的地方引用就行了，这大大增加了项目的可维护性且降低了开发难度。

ioc底层有用到反射，面试时如果可以扯一扯Java基础反射相关的东西；

![image-20210802002738276](https://gitee.com/powerfulboy/note-img/raw/master/image-20210802002738276.png)

ioc实现控制反转和依赖注入的两个阶段

![image-20210730155615101](https://gitee.com/powerfulboy/note-img/raw/master/HDAK8ul4kj7qUtB.png)

**俩种IOC容器**

* BeanFactory
  * 基础类型IOC容器，默认采用延迟初始化策略，容器启动初期速度较快，适合资源有限、功能要求不是很严格的场景。
* ApplicationContext
  * 是BeanFactory的子类，可以看作是更强大的BeanFactory
  * 提供了除beanfactory之外的高级特性，如事件发布、国际化信息支持等。
  * 系统资源要求较高，启动时完成所以初始化。



#### AOP

面向切面编程，是面向对象开发的一种补充，指不改变原来模型的基础上动态的修改模型以满足新的需求，如：动态的增加日志、权限等。AOP使业务逻辑各部分间的耦合度降低，提高程序可重用性，提高开发效率。

能够将那些与业务无关，**却为业务模块所共同调用的逻辑或责任（例如事务处理、日志管理、权限控制等）封装起来**，便于**减少系统的重复代码**，**降低模块间的耦合度**，并**有利于未来的可拓展性和可维护性**。

使用aop之后将一些通用的功能抽象出来，在需要用到的地方直接使用即可，这样就大大的简化了的代码量，增加新功能也很方便，日志功能和事务管理都使用了aop。

AOP是由**代理模式实现**的（利用**反射和动态编译**将代理模式变成动态的），在java中采用的是JDK动态代理模式，但是JDK动态代理只适合代理接口而不适合代理类(这是因为对象分为代理对象和被代理对象嘛。如果代理一个类，那么代理对象也会继承类的字段，然后这些字段是实际上是不会被使用的，就比较浪费，所以只会代理接口)

**AOP实现原理简述**：Spring AOP同时支持CGLIB和JDK动态代理，它们是来回切换的

有实现接口的情况为JDK动态代理，横向抽取机制

没实现接口的情况为cglib动态代理

### aop里边的注解

1、@Aspect
名称：@Aspect

类型：注解

位置：类定义上方

作用：设置当前类为切面类

格式：
@Aspect
public class AopAdvice {
}

说明：一个beans标签中可以配置多个aop:config标签

2、@Pointcut
名称：@Pointcut

类型：注解

位置：方法定义上方

作用：使用当前方法名作为切入点引用名称

说明：被修饰的方法忽略其业务功能，格式设定为无参无返回值的方法，方法体内空实现（非抽象）

3、@Before
名称：@Before

类型：注解

位置：方法定义上方

作用：标注当前方法作为前置通知

4、@After
名称：@After

类型：注解

位置：方法定义上方

作用：标注当前方法作为后置通知

5、@AfterReturning
名称：@AfterReturning

类型：注解

位置：方法定义上方

作用：标注当前方法作为返回后通知

returning ：设定使用通知方法参数接收返回值的变量名

6、@AfterThrowing
名称：@AfterThrowing

类型：注解

位置：方法定义上方

作用：标注当前方法作为异常后通知

throwing ：设定使用通知方法参数接收原始方法中抛出的异常对象名

7、@Around
名称：@Around

类型：注解

位置：方法定义上方

作用：标注当前方法作为环绕通知

### spring设计模式

https://zhidao.baidu.com/question/1674417593883168307.html

**1.工厂模式**，这个很明显，在各种BeanFactory以及ApplicationContext创建中都用到了；
**2.模版模式**，这个也很明显，在各种BeanFactory以及ApplicationContext实现中也都用到了；
**3.代理模式**，在Aop实现中用到了JDK的动态代理；
**4.策略模式**，第一个地方，加载资源文件的方式，使用了不同的方法，比如：ClassPathResourece，FileSystemResource，ServletContextResource，UrlResource但他们都有共同的借口Resource；第二个地方就是在Aop的实现中，采用了两种不同的方式，JDK动态代理和CGLIB代理；
**5.单例模式**，这个比如在创建bean的时候。

### Spring注解

**@Component**

**作用：**调用无参构造创建一个bean对象，并把对象存入spring的IOC容器，交由spring容器进行管理。相当于在xml中配置一个bean。

**@Controller**

**作用：**作用上与@Component。一般用于表现层的注解。

**@Service**

**作用：**作用上与@Component。一般用于业务层的注解。

**@Repository**

**作用：**作用上与@Component。一般用于持久层的注解。

**@Bean**

**作用：**用于把当前方法的返回值作为bean对象存入spring的ioc容器中

**@Autowired和@Resource**

@Resource和@Autowired都是做bean的注入时使用，其实@Resource并不是Spring的注解，它的包是javax.annotation.Resource，需要导入，但是Spring支持该注解的注入。

- ==作用：==@Autowire和@Resource都是Spring支持的注解形式动态装配bean的方式。Autowire默认按照类型(byType)装配，如果想要按照名称(byName)装配，需结合@Qualifier注解使用。
- ==共同点：==两者都可以写在字段和setter方法上。两者如果都写在字段上，那么就不需要再写setter方法。
- ==不同点：==
  - @Autowired为Spring提供的注解，需要导入包org.springframework.beans.factory.annotation.Autowired;只按照byType注入。
  - @Autowired注解是按照类型（byType）装配依赖对象，默认情况下它要求依赖对象必须存在，如果允许null值，可以设置它的required属性为false。如果我们想使用按照名称（byName）来装配，可以结合@Qualifier注解一起使用。
  - @Resource默认按照ByName自动注入，由J2EE提供，需要导入包javax.annotation.Resource。@Resource有两个重要的属性：name和type，而Spring将@Resource注解的name属性解析为bean的名字，而type属性则解析为bean的类型。所以，如果使用name属性，则使用byName的自动注入策略，而使用type属性时则使用byType自动注入策略。如果既不制定name也不制定type属性，这时将通过反射机制使用byName自动注入策略。

**@RequestMapping**：是一个用来处理请求地址映射的注解，可用于类或方法上。用于类上，表示类中的所有响应请求的方法都是以该地址作为父路径。

**@RequestBody**：注解实现接收http请求的json数据，将json转换为java对象。

**@ResponseBody**：注解实现将conreoller方法返回对象转化为json对象响应给客户。

**[@GetMapping](![img](file:///C:\Users\qing  chun\AppData\Roaming\Tencent\QQ\Temp\[5UQ[BL(6~BS2JV6W}N6[%S.png)https://github.com/GetMapping)**注解用于处理HTTP GET请求，并将请求映射到具体的处理方法中。相当于是[@RequestMapping](![img](file:///C:\Users\qing  chun\AppData\Roaming\Tencent\QQ\Temp\[5UQ[BL(6~BS2JV6W}N6[%S.png)https://github.com/RequestMapping)(method=RequestMethod.GET)

[@PostMapping](![img](file:///C:\Users\qing  chun\AppData\Roaming\Tencent\QQ\Temp\[5UQ[BL(6~BS2JV6W}N6[%S.png)https://github.com/PostMapping)注解用于处理HTTP POST请求，并将请求映射到具体的处理方法中。相当于是[@RequestMapping](![img](file:///C:\Users\qing  chun\AppData\Roaming\Tencent\QQ\Temp\[5UQ[BL(6~BS2JV6W}N6[%S.png)https://github.com/RequestMapping)(method=HttpMethod.POST)

[@PutMapping](![img](file:///C:\Users\qing  chun\AppData\Roaming\Tencent\QQ\Temp\[5UQ[BL(6~BS2JV6W}N6[%S.png)https://github.com/PutMapping)注解用于处理HTTP PUT请求，并将请求映射到具体的处理方法中。相当于是[@RequestMapping](![img](file:///C:\Users\qing  chun\AppData\Roaming\Tencent\QQ\Temp\[5UQ[BL(6~BS2JV6W}N6[%S.png)https://github.com/RequestMapping)(method=HttpMethod.PUT)

[@DeleteMapping](![img](file:///C:\Users\qing  chun\AppData\Roaming\Tencent\QQ\Temp\[5UQ[BL(6~BS2JV6W}N6[%S.png)https://github.com/DeleteMapping)注解用于处理HTTP DELETE请求，并将请求映射到删除方法中。相当于是[@RequestMapping](![img](file:///C:\Users\qing  chun\AppData\Roaming\Tencent\QQ\Temp\[5UQ[BL(6~BS2JV6W}N6[%S.png)https://github.com/RequestMapping)(method=HttpMethod.DELETE)

[@PatchMapping](![img](file:///C:\Users\qing  chun\AppData\Roaming\Tencent\QQ\Temp\[5UQ[BL(6~BS2JV6W}N6[%S.png)https://github.com/PatchMapping)注解用于处理HTTP PATCH请求，并将请求映射到对应的处理方法中。相当于是[@RequestMapping](![img](file:///C:\Users\qing  chun\AppData\Roaming\Tencent\QQ\Temp\[5UQ[BL(6~BS2JV6W}N6[%S.png)https://github.com/RequestMapping)(method=HttpMethod.PATCH)

[@PathVariable](![img](file:///C:\Users\qing  chun\AppData\Roaming\Tencent\QQ\Temp\[5UQ[BL(6~BS2JV6W}N6[%S.png)https://github.com/PathVariable)注解是将方法中的参数绑定到请求URI中的模板变量上。

[@PathVariable](![img](file:///C:\Users\qing  chun\AppData\Roaming\Tencent\QQ\Temp\[5UQ[BL(6~BS2JV6W}N6[%S.png)https://github.com/PathVariable)和[@RequestParam](![img](file:///C:\Users\qing  chun\AppData\Roaming\Tencent\QQ\Temp\[5UQ[BL(6~BS2JV6W}N6[%S.png)https://github.com/RequestParam)注解一样，如果参数名与模型具有相同的名字，则不必指定索引名称。

[@RequestParam](![img](file:///C:\Users\qing  chun\AppData\Roaming\Tencent\QQ\Temp\[5UQ[BL(6~BS2JV6W}N6[%S.png)https://github.com/RequestParam)注解用于将方法的参数与Web请求的传递的参数进行绑定。

[@Controller](![img](file:///C:\Users\qing  chun\AppData\Roaming\Tencent\QQ\Temp\[5UQ[BL(6~BS2JV6W}N6[%S.png)https://github.com/Controller)是[@Component](![img](file:///C:\Users\qing  chun\AppData\Roaming\Tencent\QQ\Temp\[5UQ[BL(6~BS2JV6W}N6[%S.png)https://github.com/Component)注解的一个延伸，Spring会自动扫描并配置被该注解标注的类。

[@RestController](![img](file:///C:\Users\qing  chun\AppData\Roaming\Tencent\QQ\Temp\[5UQ[BL(6~BS2JV6W}N6[%S.png)https://github.com/RestController)是在Spring 4.0开始引入的，这是一个特定的控制器注解。此注解相当于[@Controller](![img](file:///C:\Users\qing  chun\AppData\Roaming\Tencent\QQ\Temp\[5UQ[BL(6~BS2JV6W}N6[%S.png)https://github.com/Controller)和[@ResponseBody](![img](file:///C:\Users\qing  chun\AppData\Roaming\Tencent\QQ\Temp\[5UQ[BL(6~BS2JV6W}N6[%S.png)https://github.com/ResponseBody)的快捷方式。

### Spring中Bean的创建流程

![](http://upload-images.jianshu.io/upload_images/6292941-18712c0427c127c6.png)

- 利用该类的构造方法来实例化得到一个对象（但是如果一个类中有多个构造方法， Spring则会进行选择，这个叫做推断构造方法，下文在详细介绍）
- 得到一个对象后，Spring会判断该对象中是否存在被@Autowired注解了的属 性，把这些属性找出来并由Spring进行赋值（依赖注入）
- 依赖注入后，Spring会判断该对象是否实现了BeanNameAware接口、 BeanClassLoaderAware接口、BeanFactoryAware接口，如果实现了，就表示当前 对象必须实现该接口中所定义的setBeanName()、setBeanClassLoader()、 setBeanFactory()方法，那Spring就会调用这些方法并传入相应的参数
- Aware回调后，Spring会判断该对象中是否存在某个方法被@PostConstruct注解 了，如果存在，Spring会调用当前对象的此方法（初始化前），上面的代码中初始化了一个adminUser的数据。
- 紧接着，Spring会判断该对象是否实现了InitializingBean接口，如果实现了，就 表示当前对象必须实现该接口中的afterPropertiesSet()方法，那Spring就会调用当前对象中的afterPropertiesSet()方法（初始化）
- 最后，Spring会判断当前对象需不需要进行AOP，如果不需要那么Bean就创建完 了，如果需要进行AOP，则会进行动态代理并生成一个代理对象做为Bean（初始化 后）
  关于第6步控制台没办法打出日志，因为初始后涉及到spring的源码操作，但是可以通过断点看一下，提一句例子中UserService是被LogAspect切面切的。

### springbean的生命周期

1. 实例化 Instantiation
2. 属性赋值 Populate
3. 初始化 Initialization
4. 销毁 Destruction

实例化 -> 属性赋值 -> 初始化 -> 销毁

首先是AbstractApplicationContext的refresh方法刷新容器，然后初始化一些Bean的扩展接口，比如BeanDefinitionRegister这种，spring.factories的处理

 然后: 是InstantiationAwareBeanPostProcessor的实例化前置接口调用，他会在构造方法之前调用，构造之后还会有MergedBeanDefinitionPostProcessor处理， 

然后: 就到属性填充阶段(populateBean)，调用InstantiationAwareBeanPostProcessor后置接口，Autowire注入处理， InstantiationAwareBeanPostProcessor的postProcessPropertyValues调用 

然后: initializeBean进行Bean初始化，包括Aware系列接口调用，BeanPostProcessor前置方法，然后是相关的初始化方法，比如@Bean的init-method、 InitializingBean，后面BeanPostProcessor后置方法 然后：后面就是使用，销毁阶段...

### spring解决循环依赖注入

- 你知道**循环依赖有哪几种形式**吗？
- 答：属性注入，构造方法注入，也可以是setter方法注入，因此对应有三种循环依赖的形式。

- 问：嗯，那你知道这几种循环依赖，哪些是没有问题的，哪些是程序会报错导致不能启动的吗？
- 答：构造依赖会报错，其他两种是不会报错的，程序是可以正常启动的。

1. `singletonObjects`，一级缓存，存储的是所有创建好了的单例Bean
2. `earlySingletonObjects`，二级缓存，完成实例化，但是还未进行属性注入及初始化的对象
3. `singletonFactories`，三级缓存，提前暴露的一个单例工厂，二级缓存中存储的就是从这个工厂中获取到的对象

1.一级缓存获取，获取不到再去二级缓存获取

*2.二级缓存获取，获取不到再去三级缓存*

3.三级缓存获取，并移动到二级缓存，并清空三级缓存

**Spring 是如何解决的循环依赖？**

Spring 为了解决单例的循环依赖问题，使用了三级缓存。其中一级缓存为单例池（`singletonObjects`），二级缓存为提前曝光对象（`earlySingletonObjects`），三级缓存为提前曝光对象工厂（`singletonFactories`）。

假设A、B循环引用，实例化 A 的时候就将其放入三级缓存中，接着填充属性的时候，发现依赖了 B，同样的流程也是实例化后放入三级缓存，接着去填充属性时又发现自己依赖 A，这时候从缓存中查找到早期暴露的 A，没有 AOP 代理的话，直接将 A 的原始对象注入 B，完成 B 的初始化后，进行属性填充和初始化，这时候 B 完成后，就去完成剩下的 A 的步骤，如果有 AOP 代理，就进行 AOP 处理获取代理后的对象 A，注入 B，走剩下的流程。

**为什么要使用三级缓存呢？二级缓存能解决循环依赖吗？**

如果没有 AOP 代理，二级缓存可以解决问题，但是有 AOP 代理的情况下，只用二级缓存就意味着所有 Bean 在实例化后就要完成 AOP 代理，这样违背了 Spring 设计的原则，Spring 在设计之初就是通过 `AnnotationAwareAspectJAutoProxyCreator` 这个后置处理器来在 Bean 生命周期的最后一步来完成 AOP 代理，而不是在实例化后就立马进行 AOP 代理。

### spring事务

##### spring事务传播行为

- ==定义：==事务传播行为用来描述由某一个事务传播行为修饰的方法被嵌套进另一个方法的时事务如何传播。

  简单来说：事务传播行为是指一个事务方法A被另一个事务方法B调用时，这个事务A应该如何处理。事务A应该在事务B中运行还是另起一个事务，这个由事务A的传播行为决定。

  事务传播属性定义`TransactionDefinition`

  ```java
  int PROPAGATION_REQUIRED = 0;
  int PROPAGATION_SUPPORTS = 1;
  int PROPAGATION_MANDATORY = 2;
  int PROPAGATION_REQUIRES_NEW = 3;
  int PROPAGATION_NOT_SUPPORTED = 4;
  int PROPAGATION_NEVER = 5;
  int PROPAGATION_NESTED = 6;
  ```

  

- 七种事务传播行为

  ![image-20210806105653157](https://gitee.com/shen1shen1/pic-md1/raw/master/image-20210806105653157.png)

参考链接：https://blog.csdn.net/weixin_48272905/article/details/108525283

##### Spring事务的隔离级别

事务隔离级别定义为`TransactionDefinition`

```
int ISOLATION_DEFAULT = -1;
int ISOLATION_READ_UNCOMMITTED = 1;
int ISOLATION_READ_COMMITTED = 2;
int ISOLATION_REPEATABLE_READ = 4;
int ISOLATION_SERIALIZABLE = 8;
```

* **ISOLATION_DEFAULT:**	使用数据库默认的隔离级别，Mysql 默认采用的**可重复读**隔离级别 Oracle 默认采用的**读取已提交**隔离级别.

* **ISOLATION_READ_UNCOMMITTED:** 最低的隔离级别，允许读取尚未提交的数据变更，**可能会导致脏读、幻读或不可重复读**
* **ISOLATION_READ_COMMITTED:** 	允许读取并发事务已经提交的数据，**可以阻止脏读，但是幻读或不可重复读仍有可能发生**
* **ISOLATION_REPEATABLE_READ:** 	对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，**可以阻止脏读和不可重复读，但幻读仍有可能发生。**
* **ISOLATION_SERIALIZABLE:** 	最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，**该级别可以防止脏读、不可重复读以及幻读**。但是这将严重影响程序的性能。通常情况下也不会用到该级别。

##### Spring事务基本配置样例

```
<aop:aspectj-autoproxy proxy-target-class="true"/>
	
<bean id="transactionManager" class="org.springframework.jdbc.datasource.DataSourceTransactionManager">
	<property name="dataSource" ref="dataSource"/>
</bean>

<tx:advice id="transactionAdvice" transaction-manager="transactionManager">
	<tx:attributes>
		<tx:method name="add*" propagation="REQUIRED" rollback-for="Exception,RuntimeException,SQLException"/>
		<tx:method name="remove*" propagation="REQUIRED" rollback-for="Exception,RuntimeException,SQLException"/>
		<tx:method name="edit*" propagation="REQUIRED" rollback-for="Exception,RuntimeException,SQLException"/>
		<tx:method name="login" propagation="NOT_SUPPORTED"/>
		<tx:method name="query*" read-only="true"/>
	</tx:attributes>
</tx:advice>

<aop:config>
	<aop:advisor advice-ref="transactionAdvice" pointcut-ref="transactionPointcut"/>
    <aop:aspect ref="dataSource">
        <aop:pointcut id="transactionPointcut" expression="execution(public * com.gupaoedu..*.service..*Service.*(..))" />
    </aop:aspect>
</aop:config>
```



### SpringMVC工作原理

![image-20210830114040862](https://gitee.com/shen1shen1/pic-md1/raw/master/img/20210830114050.png)

##### SpringMVC流程

1、 用户发送请求至前端控制器DispatcherServlet。

2、 DispatcherServlet收到请求调用HandlerMapping处理器映射器。

3、 处理器映射器找到具体的处理器(可以根据xml配置、注解进行查找)，生成处理器对象及处理器拦截器(如果有则生成)一并返回给DispatcherServlet。

4、 DispatcherServlet调用HandlerAdapter处理器适配器。

5、 HandlerAdapter经过适配调用具体的处理器(Controller，也叫后端控制器)。

6、 Controller执行完成返回ModelAndView。

7、 HandlerAdapter将controller执行结果ModelAndView返回给DispatcherServlet。

8、 DispatcherServlet将ModelAndView传给ViewReslover视图解析器。

9、 ViewReslover解析后返回具体View。

10、DispatcherServlet根据View进行渲染视图（即将模型数据填充至视图中）。

11、 DispatcherServlet响应用户。

##### 组件说明：

以下组件通常使用框架提供实现：

DispatcherServlet：作为前端控制器，整个流程控制的中心，控制其它组件执行，统一调度，降低组件之间的耦合性，提高每个组件的扩展性。

HandlerMapping：通过扩展处理器映射器实现不同的映射方式，例如：配置文件方式，实现接口方式，注解方式等。 

HandlAdapter：通过扩展处理器适配器，支持更多类型的处理器。

ViewResolver：通过扩展视图解析器，支持更多类型的视图解析，例如：jsp、freemarker、pdf、excel等。

### Spring事务失效的原因

https://zhuanlan.zhihu.com/p/101396825

1. 数据库引擎不支持数据库（5.5.5之后默认InnoDB，支持；MyISAM不支持）

2. 没有被Spring管理，即类未被加载为Bean

3. 方法不是public的，如果事务非要用在非public上，可以开启`AspectJ`代理模式

4. 自身调用问题，例如：无事务A调用有事务注解的B,事务不生效；方法A上加了 `@Transactional`，B加了 `REQUIRES_NEW` 新开启一个事务，A调用B,B新开的事务不会生效；因为他们发生了自身调用，就调用该类自己的方法，而没有经过Spring的代理类，默认只有在外部调用事务才会生效

5. 数据源没有配置事务管理器

   ```
   @Bean
   public PlatformTransactionManager transactionManager(DataSource dataSource) {
       return new DataSourceTransactionManager(dataSource);
   }
   ```

   

6. 不支持事务； @Transactional(propagation = Propagation.NOT_SUPPORTED)

7. 未抛出异常

8. 异常类型错误；默认回滚`RuntimeExecptin`，想触发其他异常的回滚，需要配置注解，例如`@Transactional(rollbackFor = Exception.class)`

### JAVA_JDBC生命周期

**导jar包后：**

**1：加载驱动**
Class.forname(“全类名/jar包中Driver的相对路径：com.mysql.jdbc.Driver”)

**2：创建连接数据库对象**
DriverManager.getconnection()

**3：创建操作数据库对象**
connnection对象.createstatement()

**4：编写SQL语句**
“select * from 表名”

**5：执行SQL语句**
statement对象.extcutequery(sql)

（查询的情况下：
6：接收结果集，并操作结果集
ResultSet rs=statement对象.extcutequery(sql)
）

7：施放资源
statement对象.close()
connnection对象.close()

## SpringBoot

### SpringBoot注解

**1、@SpringBootApplication**

这个注解是Spring Boot最核心的注解，用在 Spring Boot的主类上，标识这是一个 Spring Boot 应用，用来开启 Spring Boot 的各项能力。实际上这个注解是@Configuration,@EnableAutoConfiguration,@ComponentScan三个注解的组合。由于这些注解一般都是一起使用，所以Spring Boot提供了一个统一的注解@SpringBootApplication。

**2、@EnableAutoConfiguration**

允许 Spring Boot 自动配置注解，开启这个注解之后，Spring Boot 就能根据当前类路径下的包或者类来配置 Spring Bean。

如：当前类路径下有 Mybatis 这个 JAR 包，MybatisAutoConfiguration 注解就能根据相关参数来配置 Mybatis 的各个 Spring Bean。

@EnableAutoConfiguration实现的关键在于引入了AutoConfigurationImportSelector，其核心逻辑为selectImports方法，逻辑大致如下：

　●　从配置文件META-INF/spring.factories加载所有可能用到的自动配置类；

　●　去重，并将exclude和excludeName属性携带的类排除；

　●　过滤，将满足条件（@Conditional）的自动配置类返回；

**3、@Configuration**

用于定义配置类，指出该类是 Bean 配置的信息源，相当于传统的xml配置文件，一般加在主类上。如果有些第三方库需要用到xml文件，建议仍然通过@Configuration类作为项目的配置主类——可以使用@ImportResource注解加载xml配置文件。

**4、@ComponentScan**

组件扫描。让spring Boot扫描到Configuration类并把它加入到程序上下文。

@ComponentScan注解默认就会装配标识了@Controller，@Service，@Repository，@Component注解的类到spring容器中。

**5、@Repository**

用于标注数据访问组件，即DAO组件。

使用@Repository注解可以确保DAO或者repositories提供异常转译，这个注解修饰的DAO或者repositories类会被ComponetScan发现并配置，同时也不需要为它们提供XML配置项。

**6、@Service**

一般用于修饰service层的组件

**7、@RestController**

用于标注控制层组件(如struts中的action)，表示这是个控制器bean,并且是将函数的返回值直 接填入HTTP响应体中,是REST风格的控制器；它是@Controller和@ResponseBody的合集。

**8、@ResponseBody**

表示该方法的返回结果直接写入HTTP response body中

一般在异步获取数据时使用，在使用@RequestMapping后，返回值通常解析为跳转路径，加上@responsebody后返回结果不会被解析为跳转路径，而是直接写入HTTP response body中。比如异步获取json数据，加上@responsebody后，会直接返回json数据。

**9、@Component**

泛指组件，当组件不好归类的时候，我们可以使用这个注解进行标注。

**10、@Bean**

相当于XML中的<bean></bean>,放在方法的上面，而不是类，意思是产生一个bean,并交给spring管理。

**11、@AutoWired**

byType方式。把配置好的Bean拿来用，完成属性、方法的组装，它可以对类成员变量、方法及构造函数进行标注，完成自动装配的工作。

当加上（required=false）时，就算找不到bean也不报错。

**12、@Qualifier**

当有多个同一类型的Bean时，可以用@Qualifier("name")来指定。与@Autowired配合使用

**13、@Resource(name="name",type="type")**

没有括号内内容的话，默认byName。与@Autowired干类似的事。

**14、@RequestMapping**

RequestMapping是一个用来处理请求地址映射的注解；提供路由信息，负责URL到Controller中的具体函数的映射，可用于类或方法上。用于类上，表示类中的所有响应请求的方法都是以该地址作为父路径。

**15、@RequestParam**

用在方法的参数前面。例：

```
@RequestParam String a =request.getParameter(``"a"``)。
```

**16、@PathVariable**

路径变量。参数与大括号里的名字一样要相同。例：

```
RequestMapping(``"user/get/mac/{macAddress}"``)`
    `public String getByMacAddress(@PathVariable String macAddress){``　　`
    `//do something;``
}
```

**17、@Profiles**

Spring Profiles提供了一种隔离应用程序配置的方式，并让这些配置只能在特定的环境下生效。

任何@Component或@Configuration都能被@Profile标记，从而限制加载它的时机。

```
@Configuration``@Profile(``"prod"``)
``public class ProductionConfiguration {``  
``// ...`
`}
```

**18、@ConfigurationProperties**

Spring Boot可使用注解的方式将自定义的properties文件映射到实体bean中，比如config.properties文件。

```
@Data`
`@ConfigurationProperties(``"rocketmq.consumer"``)`
`public class RocketMQConsumerProperties extends RocketMQProperties {`` 
        ``private boolean enabled = ``true``;``  
        ``private String consumerGroup;``  
        ``private MessageModel messageModel = MessageModel.CLUSTERING;``  
        ``private ConsumeFromWhere consumeFromWhere = ConsumeFromWhere.CONSUME_FROM_LAST_OFFSET;``  
        ``private int consumeThreadMin = 20;``  
        ``private int consumeThreadMax = 64;``  
        ``private int consumeConcurrentlyMaxSpan = 2000;``  
        ``private int pullThresholdForQueue = 1000;``  
        ``private int pullInterval = 0;``  
        ``private int consumeMessageBatchMaxSize = 1;``  
        ``private int pullBatchSize = 32;
	``}
```

### 简述SpringBoot框架

2.1什么是SpringBoot

Spring Boot 是所有基于 Spring 开发的项目的起点。Spring Boot 的设计是为了让你尽可能快的跑起来 Spring 应用程序并且尽可能减少你的配置文件。简单来说就是SpringBoot其实不是什么新的框架，它默认配置了很多框架的使用方式，就像maven整合了所有的jar包，spring boot整合了所有的框架（不知道这样比喻是否合适）。

### 2.2、SpringBoot四个主要特性

1、SpringBoot Starter：他将常用的依赖分组进行了整合，将其合并到一个依赖中，这样就可以一次性添加到项目的Maven或Gradle构建中；

2、自动配置：SpringBoot的自动配置特性利用了Spring4对条件化配置的支持，合理地推测应用所需的bean并自动化配置他们；

3、命令行接口：（Command-line-interface, CLI）：SpringBoot的CLI发挥了Groovy编程语言的优势，并结合自动配置进一步简化Spring应用的开发；

4、Actuatir：它为SpringBoot应用的所有特性构建一个小型的应用程序。但首先，我们快速了解每项特性，更好的体验他们如何简化Spring编程模型。

### Spring Boot中自动装配机制的原理

自动装配，简单来说就是自动把第三方组件的Bean装载到Spring IOC器里面，不需要开发人员再去写Bean的装配配置。

在Spring Boot应用里面，只需要在启动类加上@SpringBootApplication注解就可以实现自动装配。

@SpringBootApplication是一个复合注解，真正实现自动装配的注解是@EnableAutoConfiguration。

自动装配的实现主要依靠三个核心关键技术。

\1. 引入Starter启动依赖组件的时候，这个组件里面必须要包含@Configuration配置类，在这个配置类里面通过@Bean注解声明需要装配到IOC容器的Bean对象。
\2. 这个配置类是放在第三方的jar包里面，然后通过SpringBoot中的约定优于配置思想，把这个配置类的全路径放在classpath:/META-INF/spring.factories文件中。这样SpringBoot就可以知道第三方jar包里面的配置类的位置，这个步骤主要是用到了Spring里面的SpringFactoriesLoader来完成的。
\3. SpringBoot拿到所第三方jar包里面声明的配置类以后，再通过Spring提供的ImportSelector接口，实现对这些配置类的动态加载。

在我看来，SpringBoot是约定优于配置这一理念下的产物，所以在很多的地方，都会看到这类的思想。它的出现，让开发人员更加聚焦在了业务代码的编写上，而不需要去关心和业务无关的配置。

其实，自动装配的思想，在SpringFramework3.x版本里面的@Enable注解，就有了实现的雏形。@Enable注解是模块驱动的意思，我们只需要增加某个@Enable注解，就自动打开某个功能，而不需要针对这个功能去做Bean的配置，@Enable底层也是帮我们去自动完成这个模块相关Bean的注入。

### 2.3 SpringBoot开发的具体好处

回顾我们之前的 SSM 项目，搭建过程还是比较繁琐的，需要：

1、配置web.xml，加载spring和spring mvc

2、配置数据库连接、配置spring事务

3、配置加载配置文件的读取，开启注解

。。。

配置完成之后部署tomcat 调试

而使用 Spring Boot 来开发项目则只需要非常少的几个配置就可以搭建起来一个 Web 项目，并且利用 IDEA 可以自动生成生成，这简直是太爽了...

## Mybatis

### 什么是MyBatis？

MyBatis前身叫ibatis是基于Java的数据持久层框架，它内部封装了JDBC，使开发者只需要关注SQL语句本身，而不需要花费精力去处理加载驱动、创建连接、创建statement等繁杂的过程。MyBatis通过xml或注解的方式将要执行的各种statement配置起来，并通过java对象和statement中SQL的动态参数进行映射生成最终执行的SQL语句，最后由MyBatis框架执行SQL并将结果映射为Java对象并返回。MyBatis 的本质就是 Java 对数据库的操作。

MyBatis虽然实现了JPA但是它并不是一个完完全全的ORM组件，而是一个基于SQL开发的半ORM组件。

### MyBatis的生命周期？

1、加载mybatis的配置文件（也加载关联的映射文件）

InputStream inputStream = Resources.getResourceAsStream("mybatis-config.xml");

2、构建SqlSessionFactory工厂

SqlSessionFactory factory = new SqlSessionFactoryBuilder().build(inputStream);

SqlSessionFactory 的生命周期存在于整个 MyBatis 的应用之中，一旦创建了 SqlSessionFactory，就要长期保存它，直至不再使用 MyBatis 应用。
SqlSessionFactoryBuilder 的作用在于创建 SqlSessionFactory，创建成功后它就失去了作用，所以 SqlSessionFactoryBuilder 不需要长期存在，只作用于创建 SqlSessionFactory 的方法中即可。

3、创建能执行映射文件的SqlSession

SqlSession sqlSession = factory.openSession();

SqlSession 应该存活在一个业务请求中，处理完整个请求后，应该关闭这条连接，让它归还给 SqlSessionFactory，否则数据库资源就很快被耗费精光，系统就会瘫痪，所以用 try...catch...finally...语句来保证其正确关闭。

4、创建 SQL Mapper 接口，执行sqlSession.insert等方法

User user = new User("李四", "qqq", "小李", "lisi@qq.com");
sqlSession.insert("com.mybatis.pojo.User.add", user);
sqlSession.commit(); // 事务提交
sqlSession.close(); // 关闭

SQL Mapper 接口由 SqlSession 所创建，它的生命周期应该小于等于 SqlSession 的生命周期。 Mapper 代表的是一个请求中的业务处理 ，所以它应该在一个请求中，一旦处理完了相关的业务，就应该废弃它。

### 解决列名（表中的字段名称）和实体类中的属性名不一致

**方法一：使用别名的形式**

sql层面解决，直接as起别名

![image-20220324083203446](https://gitee.com/shen1shen1/pic-md1/raw/master/img/20220324083212.png)

**方法二：使用resultMap映射实体类属性名和表的字段名一一对应关系**

resultType可以指定将查询结果映射为pojo，但需要pojo的属性名和sql查询的列名一致方可映射成功。

如果sql查询字段名和pojo的属性名不一致,可以通过resultMap将字段名和属性名作一个对应关系,resultMap实质上还需要将查询结果映射到pojo对象中。

resultMap可以实现将查询结果映射为复杂类型的pojo，比如在查询结果映射对象中包括pojo和list实现一对一查询和一对多查询。

**方法三：在核心配置文件中启用下划线与驼峰式命名规则的映射** 

mapUnderscoreToCamelCase：是否启用下划线与驼峰式命名规则的映射（如first_name => firstName） 

### ResultType和ResultMap的区别

- 对象不同
  - resultmap:resultMap如果查询出来的列名和pojo的属性名不一致，通过定义一个resultMap对列名和pojo属性名之间作一个映射关系。
  - resulttype：resultType使用resultType进行输出映射，只有查询出来的列名和pojo中的属性名一致，该列才可以映射成功。
- 描述不同
  - resultmap：resultMap对于一对一表连接的处理方式通常为在主表的pojo中添加嵌套另一个表的pojo，然后在mapper.xml中采用association节点元素进行对另一个表的连接处理。
  - resulttype：resultType无法查询结果映射到pojo对象的pojo属性中，根据对结构集查询遍历的需要选择使用resultType还是resultMap。
- 类型适用不同
  - resultmap：mybatis中在查询进行select映射的时候，返回类型可以用resultType，也可以用resultMap。
  - resulttype：resultType是直接表示返回类型的,而resultMap则是对外部ResultMap的引用，但是resultType跟resultMap不能同时存在。

### mybatis怎么实现用接口从数据库查数据

- 通过动态代理调用代理对象的方法。
- 通过sqlSession执行sql操作的方法：insert|delete|select|update
- 利用Executor对象对其他三大对象进行调度。
- PreparedStatementHandler对sql进行预编译，并进行了基础配置，接着设置参数，并执行sql语句。
- ParameterHandler负责对参数进行设置，其中TypeHandler负责数据库类型和javabean类型的映射。
- 最后查询结果由ResultHandler封装。

### #{}和${}的区别

1.#将传入的数据都当成一个字符串，会对自动传入的数据加一个双引号。如：order by #user_id#，如果传入的值是111,那么解析成sql时的值为order by "111", 如果传入的值是id，则解析成的sql为order by "id".

2.$将传入的数据直接显示生成在sql中。如：orderby将传入的数据直接显示生成在sql中。如：orderbyuser_id$，如果传入的值是111,那么解析成sql时的值为order by user_id, 如果传入的值是id，则解析成的sql为order by id.

3.#方式能够很大程度防止sql注入。

4.$方式无法防止Sql注入。

5.$方式一般用于传入数据库对象，例如传入表名.

6.一般能用#的就别用$.

MyBatis排序时使用order by 动态参数时需要注意，用$而不是#

### MyBatis二级缓存

一级缓存：基于PerpetualCache的HashMap本地缓存，其作用域为Session，当Session flush或close后，该Session中的所有Cache就将清空，默认打开一级缓存。

二级缓存：与一级缓存其机制相同，默认也是采用PerpetualCache,HashMap存储，不同在于其存储作用域为Mapper(NameSpace)，并且可自定义存储源，如Ehcache。默认不打开二级缓存，要开启二级缓存，使用二级缓存属性类需要实现Serializable序列化接口（可用来保存对象的状态），可在它的映射文件中配置；

对于缓存数据更新机制，当某一个作用域（一级缓存Session/二级缓存NameSpaces）的进行了C/U/D操作后，默认该作用域下所有select中的缓存将被clear。

# Redis

### Redis持久化

https://www.redis.com.cn/redis-persistence.html

redis 提供了两种持久化的方式，分别是**RDB**（Redis DataBase）和**AOF**（Append Only File）。

RDB，简而言之，就是在不同的时间点，将 redis 存储的数据生成快照并存储到磁盘等介质上；

AOF，则是换了一个角度来实现持久化，那就是将 redis 执行过的所有写指令记录下来，在下次 redis 重新启动时，只要把这些写指令从前到后再重复执行一遍，就可以实现数据恢复了。

其实 RDB 和 AOF 两种方式也可以同时使用，在这种情况下，如果 redis 重启的话，则会优先采用 AOF 方式来进行数据恢复，这是因为 AOF 方式的数据恢复完整度更高。

如果你没有数据持久化的需求，也完全可以关闭 RDB 和 AOF 方式，这样的话，redis 将变成一个纯内存数据库，就像 memcache 一样。

### 为什么要用 redis/为什么要用缓存

主要从“高性能”和“高并发”这两点来看待这个问题。

**高性能：**

假如用户第一次访问数据库中的某些数据。这个过程会比较慢，因为是从硬盘上读取的。将该用户访问的数据存在缓存中，这样下一次再访问这些数据的时候就可以直接从缓存中获取了。操作缓存就是直接操作内存，所以速度相当快。如果数据库中的对应数据改变的之后，同步改变缓存中相应的数据即可！

**高并发：**

直接操作缓存能够承受的请求是远远大于直接访问数据库的，所以我们可以考虑把数据库中的部分数据转移到缓存中去，这样用户的一部分请求会直接到缓存这里而不用经过数据库。

### 为什么redis的zset用跳表不用红黑树

共同点：两者**插入删除，删除，查找以及迭代输出**时间复杂度红黑树和跳表的时间复杂度是一样的

跳表在区间查询的时候效率是高于红黑树的，跳表进行查找O(logn)的时间复杂度定位到区间的起点，然后在原始链表往后遍历就可以了 ，其他插入和单个条件查询，更新两者的复杂度都是相同的O(logn)
跳表的代码实现相对于红黑树更容易实现，
跳表更加灵活，他可以通过改变索引构建策略，有效平衡执行效率和内存消耗。（红黑树的平衡是通过左旋转和有旋转来进行平衡）

### redis 的线程模型

> 参考地址:https://www.javazhiyin.com/22943.html

redis 内部使用文件事件处理器 `file event handler`，这个文件事件处理器是单线程的，所以 redis 才叫做单线程的模型。它采用 IO 多路复用机制同时监听多个 socket，根据 socket 上的事件来选择对应的事件处理器进行处理。

文件事件处理器的结构包含 4 个部分：

- 多个 socket
- IO 多路复用程序
- 文件事件分派器
- 事件处理器（连接应答处理器、命令请求处理器、命令回复处理器）

多个 socket 可能会并发产生不同的操作，每个操作对应不同的文件事件，但是 IO 多路复用程序会监听多个 socket，会将 socket 产生的事件放入队列中排队，事件分派器每次从队列中取出一个事件，把该事件交给对应的事件处理器进行处理。


###  redis 和 memcached 的区别

对于 redis 和 memcached 我总结了下面四点。现在公司一般都是用 redis 来实现缓存，而且 redis 自身也越来越强大了！

1. **redis支持更丰富的数据类型（支持更复杂的应用场景）**：Redis不仅仅支持简单的k/v类型的数据，同时还提供list，set，zset，hash等数据结构的存储。memcache支持简单的数据类型，String。
2. **Redis支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用,而Memecache把数据全部存在内存之中。**
3. **集群模式**：memcached没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据；但是 redis 目前是原生支持 cluster 模式的.
4. **Memcached是多线程，非阻塞IO复用的网络模型；Redis使用单线程的多路 IO 复用模型。**


> 来自网络上的一张图，这里分享给大家！

![redis 和 memcached 的区别](http://my-blog-to-use.oss-cn-beijing.aliyuncs.com/18-9-24/61603179.jpg)



### Redis操作多个key

- 原生命令mset,mget，具有原子性

- pipeline:使用pipeline的命令个数不能太多，不然数据量过大，增加客户端的等待时间，可能会网络堵塞，可以将大量命令的拆分多个小的pipeline命令来完成。通过redis事务保证原子性。

  步骤：

  - 获取Jedis对象（一般从连接池中获取）
  - 获取Jedis对象的pipeline对象
  - 添加指令
  - 执行指令

### redis分布式锁实现原理redlock

抽奖，统计奖品数量，保证一个人抽到以后其他人能看到数量-1

#### 特性

Redis 官方站这篇文章提出了一种权威的基于 Redis 实现分布式锁的方式名叫 Redlock，此种方式比原先的单节点的方法更安全。

<!-- more -->

它可以保证以下特性：

1. 安全特性：互斥访问，即永远只有一个 client 能拿到锁
2. 避免死锁：最终 client 都可能拿到锁，不会出现死锁的情况，即使原本锁住某资源的 client crash 了或者出现了网络分区
3. 容错性：只要大部分 Redis 节点存活就可以正常提供服务

#### 怎么在单节点上实现分布式锁

> SET resource_name my_random_value NX PX 30000

主要依靠上述命令NX，该命令仅当 Key 不存在时（NX保证）set 值，并且设置过期时间 3000ms （PX保证），值 my_random_value 必须是所有 client 和所有锁请求发生期间唯一的，释放锁的逻辑是：

```lua
if redis.call("get",KEYS[1]) == ARGV[1] then
    return redis.call("del",KEYS[1])
else
    return 0
end
```

上述实现可以避免释放另一个client创建的锁，如果只有 del 命令的话，那么如果 client1 拿到 lock1 之后因为某些操作阻塞了很长时间，此时 Redis 端 lock1 已经过期了并且已经被重新分配给了 client2，那么 client1 此时再去释放这把锁就会造成 client2 原本获取到的锁被 client1 无故释放了，但现在为每个 client 分配一个 unique 的 string 值可以避免这个问题。至于如何去生成这个 unique string，方法很多随意选择一种就行了。

#### Redlock 算法

算法很易懂，起 5 个 master 节点，分布在不同的机房尽量保证可用性。为了获得锁，client 会进行如下操作：

1. 得到当前的时间，微秒单位
2. 尝试顺序地在 5 个实例上申请锁，当然需要使用相同的 key 和 random value，这里一个 client 需要合理设置与 master 节点沟通的 timeout 大小，避免长时间和一个 fail 了的节点浪费时间
3. 当 client 在大于等于 3 个 master 上成功申请到锁的时候，且它会计算申请锁消耗了多少时间，这部分消耗的时间采用获得锁的当下时间减去第一步获得的时间戳得到，如果锁的持续时长（lock validity time）比流逝的时间多的话，那么锁就真正获取到了。
4. 如果锁申请到了，那么锁真正的 lock validity time 应该是 origin（lock validity time） - 申请锁期间流逝的时间
5. 如果 client 申请锁失败了，那么它就会在少部分申请成功锁的 master 节点上执行释放锁的操作，重置状态

#### 失败重试

如果一个 client 申请锁失败了，那么它需要稍等一会在重试避免多个 client 同时申请锁的情况，最好的情况是一个 client 需要几乎同时向 5 个 master 发起锁申请。另外就是如果 client 申请锁失败了它需要尽快在它曾经申请到锁的 master 上执行 unlock 操作，便于其他 client 获得这把锁，避免这些锁过期造成的时间浪费，当然如果这时候网络分区使得 client 无法联系上这些 master，那么这种浪费就是不得不付出的代价了。

#### 放锁

放锁操作很简单，就是依次释放所有节点上的锁就行了

#### 性能、崩溃恢复和 fsync

如果我们的节点没有持久化机制，client 从 5 个 master 中的 3 个处获得了锁，然后其中一个重启了，这是注意 **整个环境中又出现了 3 个 master 可供另一个 client 申请同一把锁！** 违反了互斥性。如果我们开启了 AOF 持久化那么情况会稍微好转一些，因为 Redis 的过期机制是语义层面实现的，所以在 server 挂了的时候时间依旧在流逝，重启之后锁状态不会受到污染。但是考虑断电之后呢，AOF部分命令没来得及刷回磁盘直接丢失了，除非我们配置刷回策略为 fsnyc = always，但这会损伤性能。解决这个问题的方法是，当一个节点重启之后，我们规定在 max TTL 期间它是不可用的，这样它就不会干扰原本已经申请到的锁，等到它 crash 前的那部分锁都过期了，环境不存在历史锁了，那么再把这个节点加进来正常工作。

### 缓存穿透

> 当大量的请求无命中缓存、直接请求到后端数据库(业务代码的 bug、或恶意攻击)，同时后端数据库也没有查询到相应的记录、无法添加缓存。  
> 这种状态会一直维持，流量一直打到存储层上，无法利用缓存、还会给存储层带来巨大压力。

#### 解决方案

1. 请求无法命中缓存、同时数据库记录为空时在缓存添加该 key 的空对象(设置过期时间)，缺点是可能会在缓存中添加大量的空值键(比如遭到恶意攻击或爬虫)，而且缓存层和存储层数据短期内不一致；
2. 使用布隆过滤器在缓存层前拦截非法请求、自动为空值添加黑名单(同时可能要为误判的记录添加白名单).但需要考虑布隆过滤器的维护(离线生成/ 实时生成)。

### 缓存雪崩

> 缓存崩溃时请求会直接落到数据库上，很可能由于无法承受大量的并发请求而崩溃，此时如果只重启数据库，或因为缓存重启后没有数据，新的流量进来很快又会把数据库击倒。

#### 出现后应对

- 事前：Redis 高可用，主从 + 哨兵，Redis Cluster，避免全盘崩溃。
- 事中：本地 ehcache 缓存 + hystrix 限流 & 降级，避免数据库承受太多压力。
- 事后：Redis 持久化，一旦重启，自动从磁盘上加载数据，快速恢复缓存数据。

#### 请求过程

1. 用户请求先访问本地缓存，无命中后再访问 Redis，如果本地缓存和 Redis 都没有再查数据库，并把数据添加到本地缓存和 Redis；
2. 由于设置了限流，一段时间范围内超出的请求走降级处理(返回默认值，或给出友情提示)。

### redis缓存更新策略

1. LRU（最近最少使用，首先淘汰最长时间未被使用的页面）/LFU(最近不常用 淘汰一定时间内不常使用的页面)/FIFO(先进出去)算法的剔除：例如maxmemory-policy
2. 超时剔除：例如expire
3. 主动更新：开发控制生命周期

![image-20220316170501120](https://gitee.com/shen1shen1/pic-md1/raw/master/img/20220316170503.png)

低一致性：最大内存和淘汰策略

高一致性：超时剔除和主动更新结合，最大内存和淘汰策略兜底

### Redis定时器实现原理

1.一个循环体循环检查一个链表

2.链表中的节点主要包含时间戳，回调函数

3.每次循环获取当前系统时间，并与链表节点的时间戳对比

4.如果差值大于时间阈值，则执行链表节点的回调函数，并用当前系统时间更新链表节点的时间戳，然后进入下次循环

![image-20220507120045701](C:\Users\qing  chun\AppData\Roaming\Typora\typora-user-images\image-20220507120045701.png)

## Redis数据类型

string（字符串），hash（哈希），list（列表），set（集合）及zset(sorted set：有序集合)。

项目中：缓存验证码；还有集群登录的token；一般常用在需要计数的场景，比如用户的访问次数、热点文章的点赞转发数量等等。排行榜用zset;

**String（字符串）**

 String是redis中最基本的数据类型，一个key对应一个value。 

string 类型是二进制安全的。意思是 redis 的 string 可以包含任何数据。比如jpg图片或者序列化的对象。

string 类型是 Redis 最基本的数据类型，string 类型的值最大能存储 512MB。

**实战场景**

- **缓存**：经典使用场景，把常用信息，字符串，图片或者视频等信息放到redis中，redis作为缓存层，mysql做持久化层，降低mysql的读写压力。
- **计数器**：redis是单线程模型，一个命令执行完才会执行下一个，同时数据可以一步落地到其他的数据源。
- **session**：常见方案spring session + redis实现session共享，

**List（列表）**

Redis中的List其实就是链表（Redis用双端链表实现List）。你可以添加一个元素到列表的头部（左边）或者尾部（右边）。

**实战场景**

- **微博TimeLine**: 有人发布微博，用lpush加入时间轴，展示新的列表信息。
- **消息队列**

**Hash（哈希）**

Redis hash 是一个 string 类型的 field（字段） 和 value（值） 的映射表，hash 特别适合用于存储对象。 

Redis hash 是一个键值(key=>value)对集合。

**实战场景**

- **缓存**： 能直观，相比string更节省空间，的维护缓存信息，如用户信息，视频信息等 

**Set（集合）**

Redis 的 Set 是 string 类型的无序集合， 集合中不能出现重复的数据 。

集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是 O(1)。

比如：在微博应用中，可以将一个用户所有的关注人存在一个集合中，将其所有粉丝存在一个集合。Redis可以非常方便的实现如共同关注、共同粉丝、共同喜好等功能。

**实战场景**

- **标签**（tag）,给用户添加标签，或者用户给消息添加标签，这样有同一标签或者类似标签的可以给推荐关注的事或者关注的人。
- **点赞，或点踩，收藏等**，可以放到set中实现

**zset(sorted set：有序集合)**

Redis zset 和 set 一样也是string类型元素的集合,且不允许重复的成员。不同的是每个元素都会关联一个double类型的分数。redis正是通过分数来为集合中的成员进行从小到大的排序。

zset的成员是唯一的,但分数(score)却可以重复。

**实战场景**

- **排行榜**：例如小说视频等网站需要对用户上传的小说视频做排行榜，榜单可以按照用户关注数，更新时间，字数等打分，做排行。

# jvm垃圾回收

### 判断一个对象是否可被回收

#### 引用计数算法

给对象添加一个引用计数器，当对象增加一个引用时计数器加 1，引用失效时计数器减 1。引用计数为 0 的对象可被回收。

两个对象出现循环引用的情况下，此时引用计数器永远不为 0，导致无法对它们进行回收。

正因为循环引用的存在，因此 Java 虚拟机不使用引用计数算法。

#### 可达性分析算法

通过 GC Roots 作为起始点进行搜索，能够到达到的对象都是存活的，不可达的对象可被回收。

![image](https://www.pdai.tech/_images/pics/0635cbe8.png)

Java 虚拟机使用该算法来判断对象是否可被回收，在 Java 中 GC Roots 一般包含以下内容:

- 虚拟机栈中引用的对象
- 本地方法栈中引用的对象
- 方法区中类静态属性引用的对象
- 方法区中的常量引用的对象

### 引用类型（强软弱虚）

无论是通过引用计算算法判断对象的引用数量，还是通过可达性分析算法判断对象是否可达，判定对象是否可被回收都与引用有关。

Java 具有四种强度不同的引用类型。

#### 强引用

被强引用关联的对象不会被回收。

使用 new 一个新对象的方式来创建强引用。

```java
Object obj = new Object();
```

#### 软引用

被软引用关联的对象只有在内存不够的情况下才会被回收。

使用 SoftReference 类来创建软引用。

```java
Object obj = new Object();
SoftReference<Object> sf = new SoftReference<Object>(obj);
obj = null;  // 使对象只被软引用关联
```

#### 弱引用

被弱引用关联的对象一定会被回收，也就是说它只能存活到下一次垃圾回收发生之前。

使用 WeakReference 类来实现弱引用。

```java
Object obj = new Object();
WeakReference<Object> wf = new WeakReference<Object>(obj);
obj = null;
```

#### 虚引用

又称为幽灵引用或者幻影引用。一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用取得一个对象。

为一个对象设置虚引用关联的唯一目的就是能在这个对象被回收时收到一个系统通知。

使用 PhantomReference 来实现虚引用。

```java
Object obj = new Object();
PhantomReference<Object> pf = new PhantomReference<Object>(obj);
obj = null; 
```

### jvm垃圾回收算法

#### 标记-清除算法

![image](https://www.pdai.tech/_images/pics/a4248c4b-6c1d-4fb8-a557-86da92d3a294.jpg)

将存活的对象进行标记，然后清理掉未被标记的对象。

不足:

- 标记和清除过程效率都不高；
- 会产生大量不连续的内存碎片，导致无法给大对象分配内存。

#### 标记-整理

![image](https://www.pdai.tech/_images/pics/902b83ab-8054-4bd2-898f-9a4a0fe52830.jpg)

让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。

#### 复制算法

 ![image](https://www.pdai.tech/_images/pics/e6b733ad-606d-4028-b3e8-83c3a73a3797.jpg) 

将内存划分为大小相等的两块，每次只使用其中一块，当这一块内存用完了就将还存活的对象复制到另一块上面，然后再把使用过的内存空间进行一次清理。

主要不足是只使用了内存的一半。

现在的商业虚拟机都采用这种收集算法来回收新生代，但是并不是将新生代划分为大小相等的两块，而是分为一块较大的 Eden 空间和两块较小的 Survivor 空间，每次使用 Eden 空间和其中一块 Survivor。在回收时，将 Eden 和 Survivor 中还存活着的对象一次性复制到另一块 Survivor 空间上，最后清理 Eden 和使用过的那一块 Survivor。

HotSpot 虚拟机的 Eden 和 Survivor 的大小比例默认为 8:1，保证了内存的利用率达到 90%。如果每次回收有多于 10% 的对象存活，那么一块 Survivor 空间就不够用了，此时需要依赖于老年代进行分配担保，也就是借用老年代的空间存储放不下的对象。

### 垃圾回收器![经典垃圾收集器之间的关系](https://gitee.com/shen1shen1/pic-md1/raw/master/img/20220322163019.png)

#### Parallel scavenge

　　ps收集器是一个新生代收集器，与Parnew一样，是多线程的垃圾收集器，也是采用复制算法

　看起来与Parnew没什么不同，但其实他有自己的特点，他主要为能控制吞吐量而开发的一款收集器

　　吞吐量，就是CPU运行用户代码的时间与CPU运行总时间的比值，即吞吐量=运行用户代码时间／（运行用户代码时间+垃圾回收时间）

　　ps收集器的两个关键参数：MaxGCPauseMillis和GCTimeRatio

​		**MaxGCPauseMillis**：用于控制收集器最大垃圾回收时间

　　**GCTimeRatio**：用于控制吞吐量，即最大gc时间占比，例如设置参数为19，则gc时间占比为 1/（19+1），结果为5%。

#### CMS 收集器

CMS收集器是一种**以获取最短回收停顿时间**为目标的收集器。它非常符合在注重用户体验的应用上使用，老年代的。

**CMS（Concurrent Mark Sweep）收集器是 HotSpot 虚拟机第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程（基本上）同时工作。**

从名字中的**Mark Sweep**这两个词可以看出，CMS 收集器是一种 **“标记-清除”**算法实现的，它的运作过程相比于前面几种垃圾收集器来说更加复杂一些。整个过程分为四个步骤：

- **初始标记：** 暂停所有的其他线程，并记录下直接与 root 相连的对象，速度很快 ；
- **并发标记：** 同时开启 GC 和用户线程，用一个闭包结构去记录可达对象。但在这个阶段结束，这个闭包结构并不能保证包含当前所有的可达对象。因为用户线程可能会不断的更新引用域，所以 GC 线程无法保证可达性分析的实时性。所以这个算法里会跟踪记录这些发生引用更新的地方。
- **重新标记：** 重新标记阶段就是为了修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶段时间短
- **并发清除：** 开启用户线程，同时 GC 线程开始对为标记的区域做清扫。

![CMS 垃圾收集器 ](http://my-blog-to-use.oss-cn-beijing.aliyuncs.com/18-8-27/82825079.jpg)

从它的名字就可以看出它是一款优秀的垃圾收集器，主要优点：**并发收集、低停顿**。但是它有下面三个明显的缺点：

- **对 CPU 资源敏感；**
- **无法处理浮动垃圾；**
- **它使用的回收算法-“标记-清除”算法会导致收集结束时会有大量空间碎片产生。**

#### G1 收集器

**G1 (Garbage-First) 是一款面向服务器的垃圾收集器,主要针对配备多颗处理器及大容量内存的机器. 以极高概率满足 GC 停顿时间要求的同时,还具备高吞吐量性能特征.**

被视为 JDK1.7 中 HotSpot 虚拟机的一个重要进化特征。它具备一下特点：

- **并行与并发**：G1 能充分利用 CPU、多核环境下的硬件优势，使用多个 CPU（CPU 或者 CPU 核心）来缩短 Stop-The-World 停顿时间。部分其他收集器原本需要停顿 Java 线程执行的 GC 动作，G1 收集器仍然可以通过并发的方式让 java 程序继续执行。
- **分代收集**：虽然 G1 可以不需要其他收集器配合就能独立管理整个 GC 堆，但是还是保留了分代的概念。
- **空间整合**：与 CMS 的“标记--清理”算法不同，G1 从整体来看是基于“标记整理”算法实现的收集器；从局部上来看是基于“复制”算法实现的。
- **可预测的停顿**：这是 G1 相对于 CMS 的另一个大优势，降低停顿时间是 G1 和 CMS 共同的关注点，但 G1 除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为 M 毫秒的时间片段内。


G1 收集器的运作大致分为以下几个步骤：

- **初始标记**
- **并发标记**
- **最终标记**
- **筛选回收**

**G1 收集器在后台维护了一个优先列表，每次根据允许的收集时间，优先选择回收价值最大的 Region(这也就是它的名字 Garbage-First 的由来)**。这种使用 Region 划分内存空间以及有优先级的区域回收方式，保证了 G1 收集器在有限时间内可以尽可能高的收集效率（把内存化整为零）。

#### CMS和G1区别

区别一： 使用范围不一样
    CMS收集器是老年代的收集器，可以配合新生代的Serial和ParNew收集器一起使用

    G1收集器收集范围是老年代和新生代。不需要结合其他收集器使用

区别二： STW（stop-the-world）的时间
CMS收集器以最小的停顿时间为目标的收集器。

G1收集器可预测垃圾回收的停顿时间（建立可预测的停顿时间模型）

区别三： 垃圾碎片
CMS收集器是使用“标记-清除”算法进行的垃圾回收，容易产生内存碎片

G1收集器使用的是“标记-整理”算法，进行了空间整合，降低了内存空间碎片。

区别四： 垃圾回收的过程不一样
CMS收集器                      G1收集器

1. 初始标记                   1.初始标记
2. 并发标记                   2. 并发标记
3. 重新标记                   3. 最终标记
4. 并发清除                   4. 筛选回收

#### 常用垃圾收集器比较

![](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/11e9dcd0f1ee4f25836e6f1c47104c51-new-image69e1c56a-1d40-493a-9901-6efc647a01f3.png)

### 内存分配和回收策略——新生代如何进入老年代

**新生代GC（Minor GC）**

指发生在新生代的垃圾收集动作，因为Java对象大多都具备朝生夕灭的特性，所以Minor GC非常频繁，一般回收速度也比较快。

**老年代GC（Major GC/Full GC）**

指发生在老年代的GC，出现了Major GC，经常会伴随至少一次的Minor GC（但非绝对的，在Parallel Scavenge收集器的收集策略里就有直接进行Major GC的策略选择过程）。 Major GC的速度一般会比Minor GC慢10倍以上。

**对象优先在Eden分配**

大多数情况下，对象在新生代Eden区中分配，当Eden区没有足够空间进行分配时，虚拟机将发起一次Minor GC。

**大对象直接进入年老代**

大对象即需要大量连续内存空间的Java对象，如长字符串及数组。经常出现大对象导致内存还有不少空间时就提前触发垃圾收集以获取足够的连续空间来安置他们。
虚拟机提供了一个-XX：PretenureSizeThreshold参数，令大于这个设置值的对象直接在老年代分配。 这样做的目的是避免在Eden区及两个Survivor区之间发生大量的内存复制（新生代采用复制算法收集内存）。

**长期存活的对象将进入年老代**

虚拟机给每个对象定义了一个对象年龄计数器，在对象在Eden创建并经过第一次Minor GC后仍然存活，并能被Suivivor容纳的话，将会被移动到Survivor空间，并对象年龄设置为1。每经历过Minor GC，年龄就增加1岁，当到一定程度（默认15岁，可以通过参数-XXMaxTenuringThreshold设置），就将会晋升年老代。

**动态对象年龄判定**

为了更好地适应不同程序内存状况，虚拟机并不硬性要求对象年龄达到MaxTenuringThreshold才能晋升老年代，如果在Survivor空间中相同年龄所有对象大小的总和大于Survivor空间的一半，年龄大于或等于该年龄的对象就可以直接进入年老代。

**空间分配担保**

在发生Minor GC之前，虚拟机会先检查年老代最大可用的连续空间是否大于新生代所有对象的总空间，如果条件成立，那么Minor GC可以确保是安全的。

如果不成立，则虚拟机会查看HandlePromotionFailure设置值是否允许担保失败。

如果允许，那么会继续检查年老代最大可用连续空间是否大于历次晋升到年老代对象的平均大小，如果大于，将尝试进行一次Minor GC,尽管这次Minor GC是有风险的。

如果小于，或者HandlePromotionFailure设置不允许冒险，那这时候改为进行一次Full GC。

下面解释一下“冒险”是冒了什么风险，新生代使用复制收集算法，但为了内存利用率，只使用其中一个Survivor空间来作为轮换备份，因此当出现大量对象在MinorGC后仍然存活的情况（最极端的情况就是内存回收后新生代中所有对象都存活），就需要老年代进行分配担保，把Survivor无法容纳的对象直接进入老年代。

与生活中的贷款担保类似，老年代要进行这样的担保，前提是老年代本身还有容纳这些对象的剩余空间，一共有多少对象会活下来在实际完成内存回收之前是无法明确知道的，所以只好取之前每一次回收晋升到老年代对象容量的平均大小值作为经验值，与老年代的剩余空间进行比较，决定是否进行Full GC来让老年代腾出更多空间。

取平均值进行比较其实仍然是一种动态概率的手段，也就是说，如果某次Minor GC存活后的对象突增，远远高于平均值的话，依然会导致担保失败（Handle Promotion Failure）。

如果出现了HandlePromotionFailure失败，那就只好在失败后重新发起一次Full GC。 虽然担保失败时绕的圈子是最大的，但大部分情况下都还是会将HandlePromotionFailure开关打开，避免Full GC过于频繁。



### Tomcat源码

我了解过tomcat的实现原理，我给你讲一下tomcat 吧。在学tomcat之前，我只是知道怎么用它的。我第一次学的时候是学的 servlet+tomcat 的使用，当我对他使用的很熟练的时候，我开始想他们的实现原理是什么，**思考为什么servlet 项目要配合 tomcat 使用、servlet项目里为什么加了注解，它怎么和前端交互，doGet和doPost都是void 方法，为什么能给前端回信息，**这些都驱使着我去读源码理解它。**所以我开始找servlet的主方法，因为我想的是任何程序要启动都要有它的主方法，遗憾的是我在 servlet 项目里没有找到主方法。**当时我感觉很震惊，一个程序没有主方法是怎么启动的。这时候我突然想到了，**它是结合tomcat使用的，我猜想他部署在tomcat里面是不是和tomcat 合成了一个项目。如果能合成一个项目，这就说明tomcat也是一个java写的项目。**之前虽然熟练使用，但是没注意过tomcat是不是java写的，所以我赶紧去确认了一下，果然发现 tomcat 就是完全由java写的。所以我就认为了它确实是tomcat 和servlet 最终合成了一个项目。
为了验证我的想法，我去找 tomcat 的主方法，**那么 tomcat的主方法在哪呢？**因为之前学过 shell 脚本，所以我去找它的启动脚本，看他执行的是哪个 jar包，发现启动脚本最终导入的是catalina.sh ，我又去catalina.sh里面找 jar包，找到了bootstrap.jar，然后我用反编译工具 gui 进行反编译，**果然发现了他的主方法。**然后我开始从主方法研究它的运行原理，这是我第一次研究源码，我花了一个多月，后期我又研究了springMVC 的源码，比第一次快多了。

头一次研究源码让我进步非常大，是跨越式的进步。我通过梳理整个源码之后把tomcat分成了两个阶段，分别是启动阶段和接收转发阶段。

**启动阶段**
首先我从启动阶段开始介绍，我发现在**启动阶段，他会扫描webapp目录下的所有子目录和子文件，然后把这个目录下的 .class 文件挑选出来，拿到每个java类的类路径，拿到类路径之后我发现他用到了一个class.forName()方法去获取 Class 类信息，**当时我并不清楚这段代码是什么意思，我去查了资料才发现这个环节叫反射。从网上查资料的时候没太理解反射，往后面读了以后才发现反射是框架里的重要环节。**它在拿到所有的java类Class类信息之后会 for循环遍历每一个Class类信息，然后从Class类信息里去获取注解信息，拿到注解信息之后看哪些java的类信息里面有@WebServlet注解，**把这些java文件挑选出来，**挑选出来之后，发现有 newInstance() 步骤，**读到这我就不懂了，我又去网上搜newInstance是什么意思，然后发现可以通过newInstance() 生成类的实例，它代替了new操作，**为什么这些环节没办法使用直接new的方式而是要用newInstance() 呢，使用 new 的前提是要知道类的名字和它的包路径，很可惜tomcat的开发者事先不知道使用者创建的 servlet 叫什么名字以及所在包是什么，所以 new是不能使用的，才有了newInstance()  的方式生成实例。**读到了这我隐隐约约感觉到好像所有的框架都可能有这一层。**生成servlet实例的时候，又进一步通过方法的实例拿到了它里面的 method实例，method实例里面挑选出 doGet 和 doPost实例**，同时上面也**拿到了注解里的路径值，然后把它们放到了 hashMap 中，其中 key	值就是注解里的路径，value值就是对象实例信息。**除了method实例，servlet实例也存在hashMap中，method实例和servlet实例对应的key值都是注解的路径。



**请求转发阶段**
接下来介绍请求分发部分。**tomcat负责接收来自网络的请求，tomcat 是通过socket监听端口**，我读源码的时候发现 这里的socket 和之前学的不一样，后来发现这里引用的是netty框架中的socket （我把netty框架列入了学习计划中，打算在未来半年把netty看完）。**当tomcat 借助socket拿到http请求后并不是立即开启线程处理的，而是把它放入线程池中，**也从这我首次接触到了线程池，为了攻克这一环节我又钻研了线程池，把线程池的作用和原理都看了一遍，
**在这为什么使用线程池呢？**主要有以下几个原因：
1、**如果没有线程池，而是每个请求来了之后立刻开启线程对请求进行处理，如果一瞬间来上千个请求的话，会瞬间开启很多线程把内存完全消耗，导致服务器死机。**线程池在过多请求来的时候会把请求打到队列中，而不是立刻开启线程执行。根据线程池原理，如果队列满了还可以执行饱和策略，过多的请求不在处理，不会导致服务器宕机。
2、**如果没有线程池，每个请求来了都要新建线程，线程执行完毕后销毁。随着请求的不断到来，会不断的有线程的新建和销毁，线程的新建和销毁都是有巨大的开销的，这种开销是毫秒级的。**说道线程的新建和销毁，我又看了一下线程的新建和销毁过程，当我看过程的时候，我去查找了很多资料发现 java线程的创建和销毁都是调用的本地方法，所以我又研究了操作系统的内核，去图书馆看了linux内核源码，通过查看linux内核源码我发现原来 **线程的创建和销毁实际上实在它的栈区域申请了一个数组类型的栈结构，每个方法的执行实际上是一个个栈帧入栈的过程，方法的执行完毕是出栈的过程。**线程的创建和申请伴随着栈空间的申请和回收，而它的回收并不是立刻回收的而是批量回收的，**它的回收频率和回收次数是由java控制的。因为立刻回收会占用大量 CPU资源，会影响其他线程性能。如果不立即回收就得每隔一段时间回收，意味着回收前会堆积大量线程栈垃圾，**尤其在高并发场景下垃圾会更多。这种垃圾会占用大量内存，进而影响到整体性能。所以我们应该尽可能避免线程的新建和销毁，所以我们就考虑如何在已申请的栈空间上进行资源重复利用，线程池能让线程不会被销毁，通过读源码发现线程池里的线程 run()方法里是有死循环的，它会循环从队列里获取任务，所以线程池可以重复利用线程，大幅提升了多线程性能。
**接下来我钻研了每个线程做的工作，首先把通过端口传过来的 http字符信息封装成 httpRequest 对象和 httpResponce对象，同时提取出请求的 URL ，把URL中的 ip、端口、项目名字去掉剩下的就是要请求的servlet地址或前端地址。**如果请求的是**前端资源**那么会根据**请求路径去相应的目录下找前端文件**，之后按照相应的编码读取里面的字符串返回给前端；**如果请求的是servlet** 那么会**根据URL去之前启动阶段的 hashMap里匹配 servlet实例和 method对象，提取出来之后执行代理**，我一开始看到代理的代码时很陌生，并不理解代理是什么东西，然后我通过网上查询和书籍阅读以及请教学长的方式学习代理，之后发现java的代理有两种模式，**一是 jdk的动态代理，还有静态代理，**在学习过程中我逐渐了解到代理是用来干什么的。因为在有些情况下我们不能直接通过new对象调用方法，所以就需要有一种技术解决这种情况，我通过查询资料发现代理正好可以解决这种问题。**我通过new对象调用方法的方式猜测出代理和这种方式有异曲同工之妙，一般调用方法的时候我们知道是哪个对象调用哪个方法和方法的参数，正好对应上了 invoke() 方法的三个参数**，tomcat通过invoke()方法完成了 servlet的调用。doGet() 和doPost() 方法都是void方法没有返回值，是通过invoke()拿到返回结果然后返回给tomcat的



### SpringMVC源码

启动流程：
**当tomcat启动之后，tomcat会自动读取web.xml配置文件（约定）。首先读取DispatcherServlet，**还有其他配置文件的路径。

**DispatcherServlet的作用**：使用springMVC时，**java和前端对接的是Controller，而tomcat并不识别Controller，无法和前端沟通，所以需要一个中介。**tomcat是支持访问servlet的，所以中介就是DispatcherServlet。**由于dispatcherServlet和controller都是第三方开发人员编写的，所以dispatcherServlet里的doGet和doPost里写上controller的相关java代码。**在web.xml里配置，目的是把所以请求打到dispatcherServlet上。交给servlet负责分发。

**读取其他配置文件目的：**是把其他配置文件合到web.xml里来。读取配置文件的方式是java的DOM方法。在配置文件中有些是javaEE自带的，有些是第三方标签。**自带的标签tomcat可以直接识别，而第三方的标签是用spring的jar包去识别。**配置文件里有spring的j启动类的包路径。当tomcat读到包路径的时候，包的路径是按约定在web.xml中注册的。tomcat在启动过程中，会用DOM操作读取这些标签，看有哪些路径。按约定，凡是在这个标签里的路径都会提取出来进行反射。反射拿到类信息之后再通过newInstance拿到类实例。再通过代理的方式对他们进行执行。执行主方法引起一连串的执行。

**在springMVC的启动类中会执行以下操作：**读取web.xml文件，用DOM操作读取自定义标签。其中自定义标签里有controller的扫描范围。**当读取扫描范围之后，springMVC去包下面扫描所有文件**。把.class文件全挑出来，**根据路径进行反射。获取每个类的class类信息，挑选出带有controller注解的类的类信息。再把类里带有requestMapping注解的方法挑出来，把类上的requestMapping注解的value值和方法上requestMapping注解的value值拼成一个url，通过newInstance获取类实例，**同时也获取方法实例，然后以url为key，以方法实例做value存入hashmap。

**请求流程：**
**tomcat接收到请求后，没办法把请求塞给controller的，所以会把请求发给Dispatcherservlet**（参照tomcat原理），接收的还是httprequest。**dispatcherServlet会从请求中提取url，根据提取的url去hashmap中进行匹配，匹配成功，拿出实例，进行代理去执行相应的Method方法。**
controller方法的入参可以是对象类型，httprequest参数如何转化成对象？借助的是java反射。将参数对象进行反射，跟请求参数进行匹配，key一样就把请求参数value赋值给对象的域的值。

